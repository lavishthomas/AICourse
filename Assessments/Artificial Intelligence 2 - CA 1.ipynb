{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignement Details\n",
    "Name        : **Lavish Thomas** <br> \n",
    "Student ID  : **L00150445** <br>\n",
    "Course      : MSc in Big Data Analytics and Artificial Intelligence <br>\n",
    "Module      : Artificial Intelligence 2 <br>\n",
    "File used   : Dracula.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing spaCy Module\n",
    "import spacy\n",
    "\n",
    "# Loading the English language library\n",
    "nlp = spacy.load(\"en_core_web_sm\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 : Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dracula text file\n",
    "\n",
    "text_file = open(\"Dracula.txt\", encoding=\"utf-8\")\n",
    "entire_text = text_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying first 1000 character which have unneccesary line breaks because conversion from PDF to text file\n",
    "entire_text [10000:10500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the line breaks with space to mitigate unneccesary line breaks\n",
    "\n",
    "entire_text = entire_text.replace('\\n', ' ')\n",
    "entire_text [10000:10500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : The '\\n' characters is replaced with spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using NLP creating the doc_object\n",
    "doc_object = nlp(entire_text)\n",
    "\n",
    "### stripping the sentences\n",
    "\n",
    "sentences = [sent.string.strip() for sent in doc_object.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The doc_object creation is part of Q2 also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( 'The number of sentences in the text file is ' + str(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2 : POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Taking a random sentance\n",
    "sentences[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_noun_chunks(sentence):\n",
    "    doc_object = nlp(sentence)\n",
    "            \n",
    "    print(f\" {'Token':{10}} {'POS Tag':{10}} {'Token Dependency':{20}} {'Explanation':{30}} {'Stop word'}\")\n",
    "    for token in doc_object:\n",
    "        print(f\" {str(token):{10}} {token.pos_:{10}} {token.dep_:{20}} {spacy.explain(token.pos_):{30}} {nlp.vocab[token.text].is_stop}\")\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_noun_chunks(sentences[200])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expression is representation style for sequence of characters which is used for pattern matching in a text based searching. It allows the programmers to search for patterns like phone number, area code etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "## searching for dates avaialble. search methods returns the position of the first match\n",
    "\n",
    "search_pattern = r'\\d\\d:\\d\\d'\n",
    "time = re.search(search_pattern, entire_text)\n",
    "print('The first time : ' + str(time))\n",
    "\n",
    "## The findall funcctions returns al the matches in the text\n",
    "\n",
    "search_pattern = r'\\d\\d:\\d\\d'\n",
    "time = re.findall(search_pattern, entire_text)\n",
    "time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "The count of each charcaters mentioned is counted using the regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_patterns = [r'Godalming', r'Morris',  r'Helsing' ,  r'Lucy' ] \n",
    "\n",
    "for pattern in search_patterns:\n",
    "    occurance = re.findall(pattern, entire_text)\n",
    "    print ('The character' , pattern , 'mentioned ' , len(occurance) , 'times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: POS Frequency\n",
    "\n",
    "POS taggging represents Part of Speech, where the we can assign the POS tags using the nlp() function of the spacy library. The spacy will assign tags such as nouns, verbs etc to each token present in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_frequency = doc_object.count_by(spacy.attrs.POS)\n",
    "# Using the \".items()\" command accesses each item in the dictionary\n",
    "print(f\" {'Tag Id':>{10}} {'Token Type':{15}} {'Token Explanation':{30}} {'Occurrences':>10} \")\n",
    "for tag_id, occurrences in POS_frequency.items():\n",
    "        print(f\" {tag_id:>{10}} {doc_object.vocab[tag_id].text:{15}} {spacy.explain(doc_object.vocab[tag_id].text):{30}} {occurrences: >{10}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 Rule based matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Matcher library\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four sets of patterns is searched: <br>\n",
    "1) long <br>\n",
    "2) remember <br>\n",
    "3) tonight <br>\n",
    "4) policestation <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match for \"longer\"\n",
    "token_match1 = [{\"LOWER\": \"longer\"}]\n",
    "# match for \"long\"\n",
    "token_match2 = [{\"LOWER\": \"long\"}]\n",
    "# match for \"longest\"\n",
    "token_match3 = [{\"LOWER\": \"longest\"}]\n",
    "\n",
    "matcher.add(\"long\", None, token_match1, token_match2, token_match3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match for \"remember\"\n",
    "token_match1 = [{\"LOWER\": \"remember\"}]\n",
    "# match for \"remembered\"\n",
    "token_match2 = [{\"LOWER\": \"remembered\"}]\n",
    "# match for \"remembering\"\n",
    "token_match3 = [{\"LOWER\": \"remembering\"}]\n",
    "\n",
    "matcher.add(\"remember\", None, token_match1, token_match2, token_match3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match for \"tonight\"\n",
    "token_match1 = [{\"LOWER\": \"tonight\"}]\n",
    "# match for \"tonight\"\n",
    "token_match3 = [{\"LOWER\": \"to\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"night\"}]\n",
    "\n",
    "matcher.add(\"tonight\", None, token_match1, token_match2, token_match3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match for \"stopword\"\n",
    "token_match1 = [{\"LOWER\": \"police station\"}]\n",
    "# match for \"stopwords\"\n",
    "token_match2 = [{\"LOWER\": \"policestation\"}]\n",
    "# match for stop-word\n",
    "token_match3 = [{\"LOWER\": \"police\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"station\"}]\n",
    "\n",
    "matcher.add(\"policestation\", None, token_match1, token_match2, token_match3 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_matches = matcher(doc_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_matches(text):\n",
    "    pd = [];\n",
    "    # find all matches within the doc object\n",
    "    token_matches = matcher(text)\n",
    "    # For each item in the token_matches provide the following\n",
    "    # match_id is the hash value of the identified token match\n",
    "    print(f\"{'Match ID':<{30}} {'String':<{15}} {'Start':{5}} {'End':{5}} {'Match word':{20}}\")\n",
    "    for match_id, start, end in token_matches:\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "        pd.append(string_id)\n",
    "        matched_span = doc_object[start:end]      \n",
    "        print(f\"{match_id:<{30}} {string_id:<{15}} {start:{5}} {end:{5}} {matched_span.text:{20}}\")\n",
    "    return pd\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = find_matches(doc_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The pattern long has occured :\" + str(counter.count('long')))\n",
    "print (\"The pattern remember has occured :\" + str(counter.count('remember')))\n",
    "print (\"The pattern tonight has occured :\" + str(counter.count('tonight')))\n",
    "print (\"The pattern policestation has occured :\" + str(counter.count('policestation')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key in matcher.vocab:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Find the the count of each words\n",
    "\n",
    "Word_frequency = doc_object.count_by(spacy.attrs.POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: Previous 5 words and next 5 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prev_next_words(text):\n",
    "    # find all matches within the doc object\n",
    "    token_matches = matcher(text)\n",
    "    # For each item in the token_matches provide the following\n",
    "    # match_id is the hash value of the identified token match\n",
    "    print(f\"{'Match ID':<{30}} {'String':<{15}} {'Start':{5}} {'End':{5}} {'Match word':{5}}\")\n",
    "    for match_id, start, end in token_matches:\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "        matched_span = doc_object[start:end]      \n",
    "        print(f\"{match_id:<{30}} {string_id:<{15}} {start:{5}} {end:{5}} {matched_span.text:{5}}\\\n",
    "        {'previous 5 words'}\\\n",
    "        {doc_object[start-5]} {doc_object[start-4]} {doc_object[start-3]} {doc_object[start-2]} {doc_object[start-1]}\\\n",
    "        {'next 3 words'}\\\n",
    "        {doc_object[end]} {doc_object[end+1]} {doc_object[end+2]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_next_words(doc_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 : Phrase matching\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PhraseMatcher library\n",
    "from spacy.matcher import PhraseMatcher\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list = [\"longer\", \"long\", \"longest\"]\n",
    "phrase_patterns = [nlp.make_doc(word) for word in phrase_list]\n",
    "phrase_matcher.add(\"long\", None, *phrase_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list = [\"remember\", \"remembered\", \"remembering\"]\n",
    "phrase_patterns = [nlp.make_doc(word) for word in phrase_list]\n",
    "phrase_matcher.add(\"remember\", None, *phrase_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list = [\"tonight\", \"to night\", \"to-night\"]\n",
    "phrase_patterns = [nlp.make_doc(word) for word in phrase_list]\n",
    "phrase_matcher.add(\"tonight\", None, *phrase_patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list = [\"policestation\", \"police-station\", \"police station\"]\n",
    "phrase_patterns = [nlp.make_doc(word) for word in phrase_list]\n",
    "phrase_matcher.add(\"policestation\", None, *phrase_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(text):\n",
    "    # find all matches within the doc object\n",
    "    token_matches = phrase_matcher(text)\n",
    "    # For each item in the token_matches provide the following\n",
    "    # match_id is the hash value of the identified token match\n",
    "    print(f\"{'Match ID':<{30}} {'String':<{15}} {'Start':{5}} {'End':{5}} {'Match word':{20}}\")\n",
    "    for match_id, start, end in token_matches:\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "        matched_span = doc_object[start:end]      \n",
    "        print(f\"{match_id:<{30}} {string_id:<{15}} {start:{5}} {end:{5}} {matched_span.text:{20}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_matches(doc_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: Previous 5 words and next 5 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prev_next_words(text):\n",
    "    # find all matches within the doc object\n",
    "    token_matches = phrase_matcher(text)\n",
    "    # For each item in the token_matches provide the following\n",
    "    # match_id is the hash value of the identified token match\n",
    "    print(f\"{'Match ID':<{30}} {'String':<{15}} {'Start':{5}} {'End':{5}} {'Match word':{5}}\")\n",
    "    for match_id, start, end in token_matches:\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "        matched_span = doc_object[start:end]      \n",
    "        print(f\"{match_id:<{30}} {string_id:<{15}} {start:{5}} {end:{5}} {matched_span.text:{5}}\\\n",
    "        {'previous 5 words'}\\\n",
    "        {doc_object[start-5]} {doc_object[start-4]} {doc_object[start-3]} {doc_object[start-2]} {doc_object[start-1]}\\\n",
    "        {'next 3 words'}\\\n",
    "        {doc_object[end]} {doc_object[end+1]} {doc_object[end+2]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_next_words(doc_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: Lemmatisation\n",
    "Lemmatization converts words in the second or third forms to their first form variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lemmatization(text_to_convert):\n",
    "    for token in text_to_convert:\n",
    "        print (f\"{token.text:{15}} {token.lemma_:{30}}\")\n",
    "random_sentence= nlp(sentences[300])        \n",
    "create_lemmatization(random_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10 : Displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(random_sentence,style = \"dep\", jupyter = True, options= {\"distance\" :50, \n",
    "                                                                         \"font\": \"Ariel\",\n",
    "                                                                         \"bg\": \"black\",\n",
    "                                                                   \"color\" : \"Red\",\n",
    "                                                                   \"arrow_stroke\" : 1,\n",
    "                                                                   \"arrow_spacing\": 50,\n",
    "                                                                   \"arrow_width\" : 5,\n",
    "                                                                   \"word_spacing\": 50,\n",
    "                                                                   \"compact\" : False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
