{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Details\n",
    "Name        : **Lavish Thomas** <br> \n",
    "Student ID  : **L00150445** <br>\n",
    "Course      : MSc in Big Data Analytics and Artificial Intelligence <br>\n",
    "Module      : Artificial Intelligence 2 <br> \n",
    "Assignment  : NLP CA 2 <br>\n",
    "File used   : **quora_questions.csv**\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Used:\n",
    "This sections explains varies libraries used in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy\n",
    "NumPy is the fundamental package for scientific computing with Python for efficient multi-dimensional container of generic data. (‘API reference — pandas 1.0.3 documentation’ 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language. (‘Overview — NumPy v1.18 Manual’ 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random\n",
    "Module to provide random number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Toolkit (nltk)\n",
    "The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language. (‘Natural Language Toolkit — NLTK 3.5 documentation’ 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lavis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language. (‘Stemming and Lemmatization in Python’ 2018) \n",
    "\n",
    "\n",
    "#### Lemmatization\n",
    "\n",
    "Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words. (‘Stemming and Lemmatization in Python’ 2018).\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit-learn\n",
    "Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer \n",
    "CountVectorizer is used to split up the reviews into a list of words(a spare matrix with count of each word).\n",
    "(‘sklearn.feature_extraction.text.CountVectorizer — scikit-learn 0.22.2 documentation’ 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation\n",
    "The latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.\n",
    "(Hoffman and Blei n.d.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. Grid Search is used to find optimal hypermeters for the models.  (‘sklearn.model_selection.GridSearchCV — scikit-learn 0.22.2 documentation’ 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultinomialNB\n",
    "The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). Multinominal (‘sklearn.naive_bayes.MultinomialNB — scikit-learn 0.22.2 documentation’ 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test split\n",
    "Split arrays or matrices into random train and test subsets.  (‘sklearn.model_selection.train_test_split — scikit-learn 0.22.2 documentation’ 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "Metrics module implements several loss, score, and utility functions to measure classification performance.  (‘Metrics and scoring: quantifying the quality of predictions — scikit-learn 0.22.2 documentation’ 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the CSV file with questions\n",
    "The files are read and loaded into a datframe using the pandas inbuild function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataframe = pd.read_csv(\"quora_questions.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dataset is 808578\n"
     ]
    }
   ],
   "source": [
    "print (\"The size of the dataset is \" + str(len(raw_dataframe)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question\n",
       "0  What is the step by step guide to invest in sh...\n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2  How can I increase the speed of my internet co...\n",
       "3  Why am I mentally very lonely? How can I solve...\n",
       "4  Which one dissolve in water quikly sugar, salt..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataframe.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting the null rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_series = pd.notnull(raw_dataframe[\"question\"])\n",
    "not_null_data_frame = raw_dataframe[bool_series]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling the data set \n",
    "Randomly **200000 questions** from the given dataset is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = not_null_data_frame.sample(n = 200000, random_state = 101).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross checking the size.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Method to find separation of slits using fresn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How do I read and find my YouTube comments?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question\n",
       "2   How can I increase the speed of my internet co...\n",
       "3   Why am I mentally very lonely? How can I solve...\n",
       "10  Method to find separation of slits using fresn...\n",
       "11        How do I read and find my YouTube comments?\n",
       "12               What can make Physics easy to learn?"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making sure there is no null lines\n",
    "data_frame[\"question\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "Stemming removes the repetition of words in various form using suffices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stemmer = SnowballStemmer(language=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed = data_frame[\"question\"].map(snowball_stemmer.stem).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>why am i mentally very lonely? how can i solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>method to find separation of slits using fresn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>how do i read and find my youtube comments?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>what can make physics easy to learn?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>what are the laws to change your status from a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>what does manipulation mean?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>how much is 30 kv in hp?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>what does it mean that every time i look at th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>what are some tips on making it through the jo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question\n",
       "2   how can i increase the speed of my internet co...\n",
       "3   why am i mentally very lonely? how can i solve...\n",
       "10  method to find separation of slits using fresn...\n",
       "11        how do i read and find my youtube comments?\n",
       "12               what can make physics easy to learn?\n",
       "14  what are the laws to change your status from a...\n",
       "16                       what does manipulation mean?\n",
       "23                           how much is 30 kv in hp?\n",
       "24  what does it mean that every time i look at th...\n",
       "25  what are some tips on making it through the jo..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "The words are transformed back to the root words which reduces the feature list (shorten the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = stemmed[\"question\"].map(wordnet_lemmatizer.lemmatize).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>why am i mentally very lonely? how can i solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Method to find separation of slits using fresn...</td>\n",
       "      <td>method to find separation of slits using fresn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How do I read and find my YouTube comments?</td>\n",
       "      <td>how do i read and find my youtube comments?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "      <td>what can make physics easy to learn?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "2   How can I increase the speed of my internet co...   \n",
       "3   Why am I mentally very lonely? How can I solve...   \n",
       "10  Method to find separation of slits using fresn...   \n",
       "11        How do I read and find my YouTube comments?   \n",
       "12               What can make Physics easy to learn?   \n",
       "\n",
       "                                           lemmatized  \n",
       "2   how can i increase the speed of my internet co...  \n",
       "3   why am i mentally very lonely? how can i solve...  \n",
       "10  method to find separation of slits using fresn...  \n",
       "11        how do i read and find my youtube comments?  \n",
       "12               what can make physics easy to learn?  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame ['lemmatized'] = lemmatized\n",
    "data_frame.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aim:\n",
    "\n",
    "To evaluate the unsupervised machine learning methods for text classification, which is suitable for the provided dataset named quora questions, which is extracted from the popular Q&A platform on the internet \"Quora\". \n",
    "\n",
    "#### Method:\n",
    "Two unsupervised machine learning methods are available in the scikit-learn library for text classification. <br>\n",
    "1)\tLatentDirichletAllocation (LDA) <br>\n",
    "2)\tNon-Negative Matrix Factorisation(NMF) <br>\n",
    "\n",
    "In this library, LDA provides the log-likelihood and proximity score to evaluate the hyperparameter setting for a model. But in the current released version of the library, the NMF does not have a scoring mechanism (‘sklearn.decomposition.NMF — scikit-learn 0.22.2 documentation’ 2020). Hence, the LDA method will be employed in this project.\n",
    "\n",
    "#### Expected output:\n",
    "The expected artefacts of this question is a dataset which will be categorised under several topics. The topics names should be selected using the probability of words in each topic based on empirical knowledge.\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of Document matrix\n",
    "\n",
    "This creates a sparse matrix which has the occurrence count of each word for each row of the data frame.<br> <br>\n",
    "**CountVectorizer** is used with options: <br> \n",
    "**max_df=0.9** which identify the highly repeated words as stop words and  <br> \n",
    "**min_df=4** which identifies the scarcely occuring words as stop words too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17151"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_df is between 0-1 or an INT\n",
    "count_vectorizer = CountVectorizer(max_df=0.90, min_df=4, stop_words=\"english\")\n",
    "doc_term_matrix = count_vectorizer.fit_transform(data_frame[\"lemmatized\"])\n",
    "len(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **17150** unique words in the corpus.\n",
    "\n",
    "--------------------------\n",
    "\n",
    "### Sample word list in the corpus \n",
    "\n",
    "First 20 words(features) are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '00am',\n",
       " '01',\n",
       " '04',\n",
       " '05',\n",
       " '06',\n",
       " '08',\n",
       " '09',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100000',\n",
       " '1000rs',\n",
       " '1000â',\n",
       " '100k',\n",
       " '100m',\n",
       " '101',\n",
       " '102']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names()[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200000x17151 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 965402 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document terms matrix is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search \n",
    "\n",
    "A dictionary is defined with the proposed parameters possible for the model to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {\"n_components\": [10,12,15,20], \"learning_decay\": [ .5, .7, .9]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LatentDirichletAllocation** function is called on the search parameters defined (plausible hyper-parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the model\n",
    "lda_comparison = LatentDirichletAllocation()\n",
    "# Init Grid Search Class\n",
    "lda_comparison = GridSearchCV(lda_comparison, param_grid=search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training/fitting\n",
    "\n",
    "Using the fit function, the model will categorise the quora questions in to **n_components** using the learning decay paramters specified in previous section for the search array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=LatentDirichletAllocation(batch_size=128,\n",
       "                                                 doc_topic_prior=None,\n",
       "                                                 evaluate_every=-1,\n",
       "                                                 learning_decay=0.7,\n",
       "                                                 learning_method='batch',\n",
       "                                                 learning_offset=10.0,\n",
       "                                                 max_doc_update_iter=100,\n",
       "                                                 max_iter=10,\n",
       "                                                 mean_change_tol=0.001,\n",
       "                                                 n_components=10, n_jobs=None,\n",
       "                                                 perp_tol=0.1,\n",
       "                                                 random_state=None,\n",
       "                                                 topic_word_prior=None,\n",
       "                                                 total_samples=1000000.0,\n",
       "                                                 verbose=0),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'learning_decay': [0.9], 'n_components': [10]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the grid search\n",
    "lda_comparison.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model\n",
    "\n",
    "The best model is the model which was trained using the hyper-parameters with the highest score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood :  -8128435.742651375\n",
      "Perplexity:  3554.036428539439\n"
     ]
    }
   ],
   "source": [
    "# Best model which gives the highest score\n",
    "best_lda_model = lda_comparison.best_estimator_      \n",
    "# Metrics - log likelihood - higher score = better\n",
    "print(\"Log likelihood : \", best_lda_model.score(doc_term_matrix))\n",
    "# Perplexity - lower = better. \n",
    "# = exp(-1 * log likelihood per word)\n",
    "print(\"Perplexity: \", best_lda_model.perplexity(doc_term_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 17151)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lda_model.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA model has **10 clusters**, and vocabulary of **17150 words**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_decay': 0.9, 'n_components': 10}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_comparison.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyper parameters with the best perplexity is **learning_decay of 0.9** and **n_components of 10**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic number extarction\n",
    "\n",
    "This transformation will give a matrix which has a array of values which give probability of each question to be of that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = best_lda_model.transform(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Words\n",
    "\n",
    "The most occuring 50 words in each Topic is printed. <br> <br>\n",
    "This is used to dervie the Topic names using empherical knowledge of the words most present in the text corpus of each topic cluster identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for topic 0 are : \n",
      "['2000']['currency']['think']['affect']['help']['economy']['employees']['self']['does']['years']['class']['know']['money']['rupee']['government']['examples']['increase']['rs']['india']['day']['going']['indian']['black']['old']['things']['1000']['500']['notes']['year']['new']\n",
      "\n",
      "Top words for topic 1 are : \n",
      "['review']['light']['important']['buy']['stay']['gain']['car']['know']['eat']['people']['support']['new']['work']['mind']['tips']['speed']['looking']['process']['food']['president']['united']['states']['rid']['hair']['india']['lose']['long']['weight']['best']['does']\n",
      "\n",
      "Top words for topic 2 are : \n",
      "['python']['2016']['easily']['song']['interesting']['life']['free']['did']['series']['visit']['tv']['java']['places']['game']['time']['favorite']['watch']['google']['start']['read']['learning']['movies']['books']['programming']['book']['movie']['language']['learn']['way']['best']\n",
      "\n",
      "Top words for topic 3 are : \n",
      "['like']['tech']['doing']['mechanical']['experience']['software']['engineer']['students']['computer']['women']['study']['university']['girls']['better']['science']['career']['student']['school']['want']['start']['business']['make']['money']['college']['india']['job']['engineering']['life']['online']['best']\n",
      "\n",
      "Top words for topic 4 are : \n",
      "['parents']['countries']['work']['really']['today']['terms']['major']['feel']['tell']['india']['mean']['non']['living']['did']['war']['person']['travel']['guy']['look']['think']['water']['live']['math']['know']['time']['possible']['girl']['does']['world']['like']\n",
      "\n",
      "Top words for topic 5 are : \n",
      "['car']['business']['company']['website']['earn']['hack']['control']['bank']['gmail']['video']['mobile']['youtube']['email']['whatsapp']['google']['password']['india']['android']['using']['mean']['phone']['app']['instagram']['number']['facebook']['money']['make']['account']['use']['does']\n",
      "\n",
      "Top words for topic 6 are : \n",
      "['development']['types']['education']['say']['time']['sleep']['come']['cons']['pros']['compare']['media']['battle']['web']['digital']['cost']['music']['think']['makes']['com']['marketing']['did']['big']['social']['country']['different']['real']['difference']['india']['does']['people']\n",
      "\n",
      "Top words for topic 7 are : \n",
      "['sim']['worst']['difference']['word']['windows']['phone']['card']['sentence']['presidential']['ve']['fat']['2017']['prepare']['used']['election']['president']['did']['hillary']['iphone']['use']['clinton']['better']['laptop']['exam']['win']['2016']['buy']['thing']['donald']['trump']\n",
      "\n",
      "Top words for topic 8 are : \n",
      "['differ']['hours']['difference']['solar']['ca']['stock']['interview']['way']['asked']['work']['word']['market']['time']['writing']['life']['answers']['skills']['does']['write']['home']['answer']['meaning']['ask']['question']['ways']['improve']['questions']['english']['quora']['good']\n",
      "\n",
      "Top words for topic 9 are : \n",
      "['dark']['culture']['know']['time']['hotel']['god']['happens']['chinese']['earth']['police']['effects']['days']['safe']['girlfriend']['don']['pakistan']['war']['india']['stop']['happen']['life']['china']['sex']['energy']['make']['like']['did']['feel']['love']['does']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "probability_list = []\n",
    "\n",
    "top_number = 30\n",
    "topic_count = 0\n",
    "\n",
    "for probability_number in best_lda_model.components_:\n",
    "    text_message = f\"Top words for topic {topic_count} are : \"\n",
    "    print(text_message)\n",
    "    for number in probability_number.argsort()[-top_number:]:\n",
    "        print([count_vectorizer.get_feature_names()[number]], end=\"\")\n",
    "        probability_list.append(number)\n",
    "    print(\"\\n\")\n",
    "    topic_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic selection\n",
    "Based on words, an arbitory topic name is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_desc_list = {0: \"Trends and Current Affairs\", \n",
    "                   1: \"Lifestyle and Economics\", \n",
    "                   2: \"Youngsters and Education\", \n",
    "                   3: \"Technology Trends\", \n",
    "                   4: \"International Affairs and Equality\", \n",
    "                   5: \"Internet , E-commerce and Economics\", \n",
    "                   6: \"Nations, International relations and Patriotism \", \n",
    "                   7: \"Banking and Social media\", \n",
    "                   8: \"Doubts, Decisions and Bias\", \n",
    "                   9: \"Culture, Relationships and Travel\"\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning the topic number with the **highest probability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = []\n",
    "# topics is a list of arrays containing \n",
    "# all index positions of words for each textfile\n",
    "for popular_index_pos in topics:\n",
    "    # The max index position in each array is retrieved\n",
    "    # and add to the topic_list list\n",
    "    topic_list.append(popular_index_pos.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment of topic numbers to the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to the dataframe\n",
    "data_frame[\"Topic number\"] = topic_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using map function the topic descriptions are added to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_no_to_topic_desc = data_frame[\"Topic number\"].map(topic_desc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition the Topic Description column to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame[\"Topic desc\"] = topic_no_to_topic_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample dataframe with question, topic number and topic description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the dataframe to a csv file for the further processing in Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv(r'quora_supervised.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Question 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aim:\n",
    "To train a machine learning model which can categorise a new question into a predefined cluster which was identified in the Q1. \n",
    "#### Method:\n",
    "For text classification model, several methods are available. Such as :<br>\n",
    "\n",
    "1) Support Vector Classifier (SVC). <br>\n",
    "2) Random forest classifier (RFC). <br>\n",
    "3) Logistic Regression Classifier (LRC). <br>\n",
    "4) Naive Bayes <br>\n",
    "\n",
    "In which Naive Bayes is implemented to achieve the classification for text since a library for the same is available in the sci-kit learn.  ('sklearn.naive_bayes.MultinomialNB — scikit-learn 0.22.2 documentation’ 2020). <br> <br>\n",
    " \n",
    "#### Justification:\n",
    "The **SVC** works using hyperplanes equivalent to the number of features for which in the text-classifier is equal to the size of the vocabulary.(‘Support Vector Machines — scikit-learn 0.22.2 documentation’ 2020)  <br> <br>\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ <br>\n",
    "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. <br> \n",
    "Use zero_division parameter to control this behavior. <br>\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ <br> <br>\n",
    "\n",
    "**Random forest classifier** has the depth of the tree to be considered for which the computational power and memory will be high in demand. (‘sklearn.ensemble.RandomForestClassifier — scikit-learn 0.22.2 documentation’ 2020) <br> <br>\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ <br>\n",
    "Low Accuracy for the model than Naive Bayes<br>\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ <br> <br> \n",
    "**Logistic Regression Classifier** will be considering a one-vs-rest (OvR) scheme, which will have iteration calculating the probability of each class will be numerous. (‘sklearn.linear_model.LogisticRegression — scikit-learn 0.22.2 documentation’ 2020)<br> \n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ <br>\n",
    "ConvergenceWarning: lbfgs failed to converge (status=1): <br>\n",
    "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. <br>\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ <br> <br>\n",
    "\n",
    "\n",
    "\n",
    "Naive Bayes is a classification method which makes use of the Bayes theorem to find the probability of an item to be of a specific class. ('Naive Bayes text classification' 2020) <br> \n",
    "Initially, the text corpus will be vectorised to find the occurrence of each word. <br>\n",
    "Afterwards, the **MultinomialNB** class will be used to train the model to classify the questions. <br>\n",
    "\n",
    "\n",
    "**Note** : The code and result for other models are given in **Appendix 1**\n",
    "\n",
    "#### Expected output:\n",
    "A model with an accuracy of at least 75% or above in classifying the questions into predefined topics.\n",
    "\n",
    "--------------\n",
    "\n",
    "#### Reading of the text file with the topic classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorised_data_frame = pd.read_csv(\"quora_supervised.csv\", encoding='utf-8')\n",
    "categorised_data_frame.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of Document matrix\n",
    "\n",
    "This creates a sparse matrix which has the occurrence count of each word for each row of the data frame.<br> <br>\n",
    "**CountVectorizer** is used with options: <br> \n",
    "**max_df=0.9** which identify the highly repeated words as stop words and  <br> \n",
    "**min_df=4** which identifies the scarcely occuring words as stop words too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_df is between 0-1 or an INT\n",
    "count_vectorizer = CountVectorizer(max_df=0.90, min_df=1, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**categorised_doc_term_matrix** is the feature list. <br>\n",
    "**target_topic** is the classification expected. <br>\n",
    "\n",
    "lemmatized column is use because it will have a reduced vocalbulary size than the original question. (To reduce repetition of pre-processing in Q1&Q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorised_doc_term_matrix = count_vectorizer.fit_transform(categorised_data_frame[\"lemmatized\"])\n",
    "target_topic = categorised_data_frame['Topic number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test split\n",
    "\n",
    "The current dataset is split into 70% training and 30% for testing the model afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(categorised_doc_term_matrix,target_topic, test_size = 0.3, random_state = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier \n",
    "MultinomialNB classifier is created and used to fit/train the model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnc_classifier = MultinomialNB()\n",
    "mnc_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "The Model is now equipped with data to predict a new review is positive or negative. A **positive review** is fed into the classifier to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnc_model_predictions = mnc_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "In order to evaluate our model for the movie review classifer we are going to use Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(y_test, mnc_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report\n",
    "\n",
    "Based on the confusion metrics the classification report can be calculated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, mnc_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Precision: \n",
    "When it predicts it is of a particular class, the percentage of the correct predictions. In this case, precision varies from 0.76 to 0.87 for various classes.\n",
    "\n",
    "#### Recall\n",
    "Recall is the number of correct results divided by the number of results that should have been returned. In this acse, it varies from 0.69 to 0.89 for various classes.\n",
    "\n",
    "#### F1-score\n",
    "The F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall). In this case, it varies from 0.76 to 0.83.\n",
    "\n",
    "#### Support\n",
    "The support is the number of occurrences of each class. The test size of 30% of dataset (60000) question were used for testing.\n",
    "\n",
    "#### Accuracy        \n",
    "The accuracy defines the percentage of times the model correctly predicted the class. 80% by this model.\n",
    "\n",
    "#### Macro avg\n",
    "The unweighted mean per topic. This is mean without considering th occurence of each class in the dataset. This susually used if the classses were of equal presence.\n",
    " \n",
    "#### Weighted avg\n",
    "Support-weighted mean per topic. This average is calculated is by considering the amount of the data present for each category.\n",
    "\n",
    "<br> \n",
    "(‘Metrics and scoring: quantifying the quality of predictions — scikit-learn 0.22.2 documentation’ 2020)\n",
    "\n",
    "--------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of 10 random questions\n",
    "10 random positions are selected from the range of question bank provided (0 to 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = random.sample(range(1, 200000), 10)\n",
    "print(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print ('-------------------------------------------')\n",
    "    print('Question : '+ categorised_data_frame.question[rand[i]])\n",
    "    #print(categorised_doc_term_matrix[rand[i]])    \n",
    "    predicted_topic_no = mnc_classifier.predict(categorised_doc_term_matrix[rand[i]])[0]\n",
    "    target_topic_no = target_topic[rand[i]]\n",
    "    print ('Predicted Topic No : ' , predicted_topic_no )\n",
    "    print ('Predicted Topic Name : ' , topic_desc_list[predicted_topic_no] )\n",
    "    print('Target Topic No : ' , target_topic_no )   \n",
    "    print('Target Topic Name : ' , topic_desc_list[target_topic_no] )   \n",
    "    \n",
    "    print ('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "\n",
    "Out of 10 random questions, 8 were predicted correctly.<br>\n",
    "\n",
    "With respect to the accuracy of the model which was calculated using the classification report (~80). <br>\n",
    "The model is consistent in predicting the Topics for new questions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Summary:\n",
    "<ol>\n",
    "  <li>The data was loaded into a Pandas Dataframe.                                                                           </li>\n",
    "<li>The null values were removed from the data frame. Afterwards, stemming and lemmatisation was applied (Pre-processing). </li>\n",
    "<li>The text data was vectorised into a form of sparse matrix representation.                                              </li>\n",
    "<li>The LDA model was selected, and a grid search was performed to find the optimal parameters.                            </li>\n",
    "<li>The data were categorised based on the best model trained with the optimal parameters.                                 </li>\n",
    "<li>The topic numbers were a topic name based on empirical analysis of the top keywords in each cluster.                   </li>\n",
    "<li>The new data frame was saved back to the hard disk for further processing.                                             </li>\n",
    "<li>This new data dataset was again transformed for a training a supervised model.                                         </li>\n",
    "<li>The Naive Bayes was selected for the classification of the new questions.                                              </li>\n",
    "<li>The dataset was split into train and test sets.                                                                       </li>\n",
    "<li>The model was trained on the training set and then evaluated based on the test set.                                   </li>\n",
    "<li>Ten random questions were selected, and the prediction was performed on the same.                                     </li>\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "API Reference — Pandas 1.0.3 Documentation [online] (2020) available: https://pandas.pydata.org/docs/reference/index.html [accessed 14 Apr 2020].                                                                                                 <br><br>\n",
    "Hoffman, M.D., Blei, D.M. (n.d.) ‘Online Learning for Latent Dirichlet Allocation’, 9.                                                                                                                                                            <br><br>\n",
    "Metrics and Scoring: Quantifying the Quality of Predictions — Scikit-Learn 0.22.2 Documentation [online] (2020) available: https://scikit-learn.org/stable/modules/model_evaluation.html [accessed 14 Apr 2020].                                  <br><br>\n",
    "Naive Bayes Text Classification [online] (2020) available: https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html [accessed 15 Apr 2020].                                                                       <br><br>\n",
    "Natural Language Toolkit — NLTK 3.5 Documentation [online] (2020) available: https://www.nltk.org/ [accessed 15 Apr 2020].                                                                                                                        <br><br>\n",
    "Overview — NumPy v1.18 Manual [online] (2020) available: https://numpy.org/doc/1.18/ [accessed 14 Apr 2020].                                                                                                                                      <br><br>\n",
    "Sklearn.Decomposition.LatentDirichletAllocation — Scikit-Learn 0.22.2 Documentation [online] (2020) available: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html [accessed 14 Apr 2020].     <br><br>\n",
    "Sklearn.Decomposition.NMF — Scikit-Learn 0.22.2 Documentation [online] (2020) available: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html [accessed 14 Apr 2020].                                                 <br><br>\n",
    "Sklearn.Ensemble.RandomForestClassifier — Scikit-Learn 0.22.2 Documentation [online] (2020) available: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html [accessed 15 Apr 2020].                     <br><br>\n",
    "Sklearn.Feature_extraction.Text.CountVectorizer — Scikit-Learn 0.22.2 Documentation [online] (2020) available: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html [accessed 14 Apr 2020].     <br><br>\n",
    "Sklearn.Linear_model.LogisticRegression — Scikit-Learn 0.22.2 Documentation [online] (2020) available: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html [accessed 15 Apr 2020].                     <br><br>\n",
    "Sklearn.Model_selection.GridSearchCV — Scikit-Learn 0.22.2 Documentation [online] (2020) available: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html [accessed 14 Apr 2020].                           <br><br>\n",
    "Sklearn.Model_selection.Train_test_split — Scikit-Learn 0.22.2 Documentation [online] (2020) available: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html [accessed 14 Apr 2020].                   <br><br>\n",
    "Sklearn.Naive_bayes.MultinomialNB — Scikit-Learn 0.22.2 Documentation [online] (2020) available: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html [accessed 14 Apr 2020].                                 <br><br>\n",
    "Stemming and Lemmatization in Python [online] (2018) DataCamp Community, available: https://www.datacamp.com/community/tutorials/stemming-lemmatization-python [accessed 15 Apr 2020].                                                            <br><br>\n",
    "Support Vector Machines — Scikit-Learn 0.22.2 Documentation [online] (2020) available: https://scikit-learn.org/stable/modules/svm.html [accessed 15 Apr 2020].                                                                                   <br><br>\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  <br><br>\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3557  134  136  241  121  147  133   67   97  118]\n",
      " [ 285 3790  215  208  209  209  207  166  126  189]\n",
      " [ 160  118 4956  187   98  155  160   99  108  100]\n",
      " [ 273  185  294 5385  186  227  194  105  148  150]\n",
      " [ 338  223  186  264 3539  195  284  108  145  417]\n",
      " [ 275  205  285  295  174 5217  202  183  169  141]\n",
      " [ 353  249  211  280  324  250 3578   78  172  327]\n",
      " [ 207  163  195  225  158  287  175 3483   95  129]\n",
      " [ 193  144  236  180  121  184  170   76 3445   97]\n",
      " [ 349  295  180  253  532  217  362  117  152 5270]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      4751\n",
      "           1       0.69      0.68      0.68      5604\n",
      "           2       0.72      0.81      0.76      6141\n",
      "           3       0.72      0.75      0.73      7147\n",
      "           4       0.65      0.62      0.63      5699\n",
      "           5       0.74      0.73      0.73      7146\n",
      "           6       0.65      0.61      0.63      5822\n",
      "           7       0.78      0.68      0.73      5117\n",
      "           8       0.74      0.71      0.73      4846\n",
      "           9       0.76      0.68      0.72      7727\n",
      "\n",
      "    accuracy                           0.70     60000\n",
      "   macro avg       0.70      0.70      0.70     60000\n",
      "weighted avg       0.71      0.70      0.70     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First import the model we want to use\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create an instance of the model - common model for text data and spam filtering\n",
    "rf_model = RandomForestClassifier(n_estimators = 10)\n",
    "\n",
    "# Fit model to training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict answers to data from the X_text dataset\n",
    "# containing text length and punctuation count\n",
    "rf_model_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Show results in a confusion matrix\n",
    "print(metrics.confusion_matrix(y_test,rf_model_predictions))\n",
    "\n",
    "print(metrics.classification_report(y_test,rf_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a Support Vector Classification model (SVC)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Setting gamma to \"auto\", otherwise the SVC model \n",
    "# returns an error\n",
    "svc_model = SVC(gamma=\"auto\")\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "svc_model_predictions = svc_model.predict(X_test)\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, svc_model_predictions))\n",
    "print(metrics.classification_report(y_test,svc_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lin_reg_model = LogisticRegression(solver='lbfgs')\n",
    "# Note that the \"fit\" option must be run in the same cell as line above\n",
    "lin_reg_model.fit(X_train, y_train)\n",
    "\n",
    "lin_reg_model_predictions = lin_reg_model.predict(X_test)\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, lin_reg_model_predictions))\n",
    "print(metrics.classification_report(y_test,lin_reg_model_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
