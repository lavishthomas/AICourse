{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignement Details\n",
    "Name        : **Lavish Thomas** <br> \n",
    "Student ID  : **L00150445** <br>\n",
    "Course      : MSc in Big Data Analytics and Artificial Intelligence <br>\n",
    "Module      : Artificial Intelligence 2 <br>\n",
    "File used   : quora_questions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Used:\n",
    "This sections explains varies libraries used in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy\n",
    "NumPy is the fundamental package for scientific computing with Python for efficient multi-dimensional container of generic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit-learn\n",
    "Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pandas\n",
    "Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CountVectorizer \n",
    "CountVectorizer is used to split up the reviews into a list of words(a spare matrix with count of each word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Latent Dirichlet Allocation\n",
    "The latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. Grid Search is used to find optimal hypermeters for the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split arrays or matrices into random train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics module implements several loss, score, and utility functions to measure classification performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the CSV file with questions\n",
    "The files are read and loaded into a datframe using the pandas inbuild function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataframe = pd.read_csv(\"quora_questions.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The size of the dataset is \" + len(raw_dataframe));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question\n",
       "0  What is the step by step guide to invest in sh...\n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2  How can I increase the speed of my internet co...\n",
       "3  Why am I mentally very lonely? How can I solve...\n",
       "4  Which one dissolve in water quikly sugar, salt..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataframe.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting the null rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_series = pd.notnull(raw_dataframe[\"question\"])\n",
    "not_null_data_frame = raw_dataframe[bool_series]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the data set \n",
    "Randomly 200000 questions from the given dataset is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = not_null_data_frame.sample(n = 200000, random_state = 100).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Cross checking the size.\n",
    "len(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sample dataset\n",
    "data_frame.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making sure there is no null lines\n",
    "data_frame[\"question\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aim:\n",
    "\n",
    "\n",
    "### Method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer is used to split up the reviews into a list of words(a spare matrix with count of each word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17151"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_df is between 0-1 or an INT\n",
    "count_vectorizer = CountVectorizer(max_df=0.90, min_df=4, stop_words=\"english\")\n",
    "doc_term_matrix = count_vectorizer.fit_transform(data_frame[\"question\"])\n",
    "len(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '001',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " '07',\n",
       " '08',\n",
       " '09',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100000',\n",
       " '1000rs',\n",
       " '1000s',\n",
       " '1000Ã¢',\n",
       " '100k']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names()[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = count_vectorizer.fit_transform(data_frame[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200000x17151 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 963944 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=LatentDirichletAllocation(batch_size=128,\n",
       "                                                 doc_topic_prior=None,\n",
       "                                                 evaluate_every=-1,\n",
       "                                                 learning_decay=0.7,\n",
       "                                                 learning_method='batch',\n",
       "                                                 learning_offset=10.0,\n",
       "                                                 max_doc_update_iter=100,\n",
       "                                                 max_iter=10,\n",
       "                                                 mean_change_tol=0.001,\n",
       "                                                 n_components=10, n_jobs=None,\n",
       "                                                 perp_tol=0.1,\n",
       "                                                 random_state=None,\n",
       "                                                 topic_word_prior=None,\n",
       "                                                 total_samples=1000000.0,\n",
       "                                                 verbose=0),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'learning_decay': [0.5, 0.7, 0.9],\n",
       "                         'n_components': [10, 12, 15, 20]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defien the params that we want to use\n",
    "search_params = {\"n_components\": [10,12,15,20], \"learning_decay\": [ .5, .7, .9]}\n",
    "\n",
    "# Init the model\n",
    "lda_comparison = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "lda_comparison = GridSearchCV(lda_comparison, param_grid=search_params)\n",
    "\n",
    "# Run the grid search\n",
    "lda_comparison.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood :  -8145658.213802085\n",
      "Perplexity:  3664.3950402612954\n"
     ]
    }
   ],
   "source": [
    "# Best model which gives the highest score\n",
    "best_lda_model = lda_comparison.best_estimator_      \n",
    "# Metrics - log likelihood - higher score = better\n",
    "print(\"Log likelihood : \", best_lda_model.score(doc_term_matrix))\n",
    "# Perplexity - lower = better. \n",
    "# = exp(-1 * log likelihood per word)\n",
    "print(\"Perplexity: \", best_lda_model.perplexity(doc_term_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 17151)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lda_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_decay': 0.5, 'n_components': 10}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_comparison.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for topic 0 are : \n",
      "['person']['blog']['fall']['woman']['distance']['traffic']['guys']['end']['man']['just']['feel']['created']['successful']['sleep']['height']['friends']['code']['eat']['hotel']['universe']['age']['really']['don']['guy']['police']['time']['ways']['way']['safe']['school']['girls']['relationship']['website']['know']['read']['energy']['want']['long']['earn']['girl']['increase']['learn']['does']['start']['online']['love']['like']['did']['money']['make']\n",
      "\n",
      "Top words for topic 1 are : \n",
      "['like']['company']['apps']['site']['mba']['management']['phone']['ve']['free']['student']['service']['sentence']['engineer']['time']['worst']['digital']['bangalore']['course']['big']['software']['2016']['better']['mind']['online']['tech']['visit']['marketing']['places']['movie']['mechanical']['career']['buy']['mobile']['delhi']['favorite']['place']['learn']['life']['app']['meaning']['books']['android']['word']['book']['thing']['job']['engineering']['india']['way']['best']\n",
      "\n",
      "Top words for topic 2 are : \n",
      "['won']['samsung']['different']['america']['drug']['jio']['videos']['center']['video']['vote']['famous']['sim']['better']['alcohol']['cell']['happen']['000']['usa']['believe']['websites']['culture']['use']['presidential']['die']['series']['does']['good']['united']['think']['states']['election']['download']['india']['tv']['phone']['2016']['youtube']['watch']['live']['number']['like']['did']['win']['iphone']['hillary']['clinton']['movies']['president']['donald']['trump']\n",
      "\n",
      "Top words for topic 3 are : \n",
      "['date']['10']['years']['differences']['buy']['cold']['likes']['advice']['better']['good']['improvement']['engine']['solve']['don']['desert']['ones']['2017']['compare']['drive']['hard']['windows']['asked']['people']['does']['friend']['employees']['girl']['answers']['best']['difference']['men']['women']['game']['answer']['car']['laptop']['average']['going']['water']['math']['day']['ask']['old']['things']['question']['new']['know']['questions']['year']['quora']\n",
      "\n",
      "Top words for topic 4 are : \n",
      "['want']['significance']['china']['japanese']['country']['exams']['2017']['got']['british']['join']['army']['physics']['cat']['general']['countries']['oil']['state']['religion']['students']['indians']['control']['marks']['pakistan']['getting']['medical']['school']['jee']['people']['difference']['period']['pregnant']['prepare']['exam']['battle']['score']['education']['american']['university']['college']['important']['think']['good']['class']['chinese']['did']['hair']['study']['com']['indian']['india']\n",
      "\n",
      "Top words for topic 5 are : \n",
      "['thinking']['2016']['eye']['corruption']['really']['happy']['demonetization']['law']['uber']['exist']['parents']['fight']['dream']['brain']['rupees']['affect']['matter']['movie']['banning']['ban']['decision']['girlfriend']['human']['note']['purpose']['biggest']['help']['currency']['2000']['new']['modi']['economy']['did']['real']['white']['stop']['rupee']['india']['money']['think']['government']['rs']['indian']['black']['1000']['500']['notes']['mean']['life']['does']\n",
      "\n",
      "Top words for topic 6 are : \n",
      "['better']['types']['page']['bad']['google']['safety']['software']['provider']['languages']['air']['worth']['open']['need']['research']['advantages']['internet']['california']['solar']['program']['machine']['time']['python']['ca']['home']['differ']['used']['password']['like']['writing']['using']['use']['account']['skills']['java']['data']['difference']['card']['web']['bank']['learning']['science']['programming']['computer']['language']['learn']['improve']['english']['work']['does']['good']\n",
      "\n",
      "Top words for topic 7 are : \n",
      "['ways']['sun']['happens']['better']['loss']['plan']['mass']['function']['star']['look']['space']['good']['use']['invest']['prepare']['years']['god']['fast']['faster']['buy']['person']['pakistan']['market']['reduce']['stock']['gain']['death']['months']['speed']['country']['exam']['fat']['light']['earth']['month']['body']['travel']['start']['happen']['possible']['india']['like']['war']['sex']['lose']['feel']['weight']['does']['time']['world']\n",
      "\n",
      "Top words for topic 8 are : \n",
      "['singapore']['model']['eating']['problems']['earthquake']['small']['recruit']['majors']['gold']['benefits']['high']['technology']['share']['stop']['people']['point']['face']['major']['product']['investment']['experience']['green']['apply']['jobs']['long']['china']['value']['use']['health']['ideas']['universities']['visa']['companies']['work']['difference']['startup']['pay']['idea']['effects']['looking']['cost']['new']['like']['rid']['good']['food']['india']['company']['business']['does']\n",
      "\n",
      "Top words for topic 9 are : \n",
      "['difference']['search']['follow']['interesting']['deleted']['photos']['correct']['movie']['deal']['way']['send']['cons']['interview']['pros']['messages']['hate']['overcome']['snapchat']['email']['recover']['profile']['say']['makes']['delete']['story']['tips']['don']['change']['using']['easily']['gmail']['hack']['quora']['history']['process']['know']['phone']['making']['social']['number']['use']['does']['examples']['person']['whatsapp']['facebook']['account']['google']['instagram']['people']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "probability_list = []\n",
    "\n",
    "top_number = 50\n",
    "topic_count = 0\n",
    "\n",
    "for probability_number in best_lda_model.components_:\n",
    "    text_message = f\"Top words for topic {topic_count} are : \"\n",
    "    print(text_message)\n",
    "    for number in probability_number.argsort()[-top_number:]:\n",
    "        print([count_vectorizer.get_feature_names()[number]], end=\"\")\n",
    "        probability_list.append(number)\n",
    "    print(\"\\n\")\n",
    "    topic_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.5,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = best_lda_model.transform(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = []\n",
    "# topics is a list of arrays containing \n",
    "# all index positions of words for each textfile\n",
    "for popular_index_pos in topics:\n",
    "    # Get the max index position in each array\n",
    "    # and add to the topic_list list\n",
    "    topic_list.append(popular_index_pos.argmax())\n",
    "\n",
    "# Add a new column to the dataframe\n",
    "data_frame[\"Topic number\"] = topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = {0: \"Education\", \n",
    "              1: \"Research\", \n",
    "              2: \"Law\", \n",
    "              3: \"Sport\", \n",
    "              4: \"Finance\", \n",
    "              5: \"Health\", \n",
    "              6: \"horoscopes\", \n",
    "              7: \"Environment\", \n",
    "              8: \"Economy\", \n",
    "              9: \"Various\", \n",
    "              10: \"Sport\", \n",
    "              }\n",
    "\n",
    "topic_no_to_topic = data_frame[\"Topic number\"].map(topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame[\"Topic desc\"] = topic_no_to_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv(r'QuoraWithTopic.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(\"QuoraWithTopic.csv\", encoding='utf-8')\n",
    "data_frame.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_df is between 0-1 or an INT\n",
    "count_vectorizer = CountVectorizer(max_df=0.90, min_df=4, stop_words=\"english\")\n",
    "doc_term_matrix = count_vectorizer.fit_transform(data_frame[\"question\"])\n",
    "len(count_vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = count_vectorizer.fit_transform(data_frame[\"question\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_topic = data_frame['Topic number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(doc_term_matrix,target_topic, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier \n",
    "MultinomialNB classifier is created and used to fit/train the model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnc_classifier = MultinomialNB()\n",
    "mnc_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "The Model is now equipped with data to predict a new review is positive or negative. A **positive review** is fed into the classifier to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnc_model_predictions = mnc_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "In order to evaluate our model for the movie review classifer we are going to use Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(y_test, mnc_model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, mnc_model_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
