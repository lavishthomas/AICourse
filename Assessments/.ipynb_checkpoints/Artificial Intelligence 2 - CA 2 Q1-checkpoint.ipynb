{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Details\n",
    "Name        : **Lavish Thomas** <br> \n",
    "Student ID  : **L00150445** <br>\n",
    "Course      : MSc in Big Data Analytics and Artificial Intelligence <br>\n",
    "Module      : Artificial Intelligence 2 <br> \n",
    "Assignment  : NLP CA 2 <br>\n",
    "File used   : **quora_questions.csv**\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Used:\n",
    "This sections explains varies libraries used in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy\n",
    "NumPy is the fundamental package for scientific computing with Python for efficient multi-dimensional container of generic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit-learn\n",
    "Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pandas\n",
    "Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer \n",
    "CountVectorizer is used to split up the reviews into a list of words(a spare matrix with count of each word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation\n",
    "The latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. Grid Search is used to find optimal hypermeters for the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultinomialNB\n",
    "The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test split\n",
    "Split arrays or matrices into random train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "Metrics module implements several loss, score, and utility functions to measure classification performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the CSV file with questions\n",
    "The files are read and loaded into a datframe using the pandas inbuild function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataframe = pd.read_csv(\"quora_questions.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The size of the dataset is \" + str(len(raw_dataframe)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataframe.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting the null rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_series = pd.notnull(raw_dataframe[\"question\"])\n",
    "not_null_data_frame = raw_dataframe[bool_series]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling the data set \n",
    "Randomly 200000 questions from the given dataset is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = not_null_data_frame.sample(n = 200000, random_state = 101).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cross checking the size.\n",
    "len(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Sample dataset\n",
    "data_frame.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure there is no null lines\n",
    "data_frame[\"question\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aim:\n",
    "\n",
    "To evaluate the unsupervised machine learning methods for text classification, which is suitable for the provided dataset named quora questions, which is extracted from the popular Q&A platform on the internet \"Quora\". \n",
    "\n",
    "#### Method:\n",
    "Two unsupervised machine learning methods are available in the scikit-learn library for text classification. <br>\n",
    "1)\tLatentDirichletAllocation (LDA) <br>\n",
    "2)\tNon-Negative Matrix Factorisation(NMF) <br>\n",
    "\n",
    "In this library, LDA provides the log-likelihood and proximity score to evaluate the hyperparameter setting for a model. But in the current released version of the library, the NMF does not have a scoring mechanism. Hence, the LDA method will be employed in this project.\n",
    "\n",
    "#### Expected output:\n",
    "The expected artefacts of this question is a dataset which will be categorised under several topics. The topics names should be selected using the probability of words in each topic based on empirical knowledge.\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of Document matrix\n",
    "\n",
    "This creates a sparse matrix which has the occurrence count of each word for each row of the data frame.<br> <br>\n",
    "**CountVectorizer** is used with options: <br> \n",
    "**max_df=0.9** which identify the highly repeated words as stop words and  <br> \n",
    "**min_df=4** which identifies the scarcely occuring words as stop words too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_df is between 0-1 or an INT\n",
    "count_vectorizer = CountVectorizer(max_df=0.90, min_df=4, stop_words=\"english\")\n",
    "doc_term_matrix = count_vectorizer.fit_transform(data_frame[\"question\"])\n",
    "len(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **17150** unique words in the corpus.\n",
    "\n",
    "--------------------------\n",
    "\n",
    "### Sample word list in the corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.get_feature_names()[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document terms matrix is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search \n",
    "\n",
    "A dictionary is defined with the proposed parameters possible for the model to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defien the params that we want to use\n",
    "search_params = {\"n_components\": [10,12,15,20], \"learning_decay\": [ .5, .7, .9]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LatentDirichletAllocation** function is called on the search parameters defined (plausible hyper-parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the model\n",
    "lda_comparison = LatentDirichletAllocation()\n",
    "# Init Grid Search Class\n",
    "lda_comparison = GridSearchCV(lda_comparison, param_grid=search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training/fitting\n",
    "\n",
    "Using the fit function, the model will categorise the quora questions in to **n_components** using the learning decay paramters specified before in search array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the grid search\n",
    "lda_comparison.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model\n",
    "\n",
    "The best model is the model which was trained using the hyper-parameters with the highest score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model which gives the highest score\n",
    "best_lda_model = lda_comparison.best_estimator_      \n",
    "# Metrics - log likelihood - higher score = better\n",
    "print(\"Log likelihood : \", best_lda_model.score(doc_term_matrix))\n",
    "# Perplexity - lower = better. \n",
    "# = exp(-1 * log likelihood per word)\n",
    "print(\"Perplexity: \", best_lda_model.perplexity(doc_term_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lda_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_comparison.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Words\n",
    "\n",
    "The most occuring 50 words in each Topic is printed. <br> <br>\n",
    "This is used to dervie the Topic names using empherical knowledge of the words most present in the text corpus of each topic cluster identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "probability_list = []\n",
    "\n",
    "top_number = 50\n",
    "topic_count = 0\n",
    "\n",
    "for probability_number in best_lda_model.components_:\n",
    "    text_message = f\"Top words for topic {topic_count} are : \"\n",
    "    print(text_message)\n",
    "    for number in probability_number.argsort()[-top_number:]:\n",
    "        print([count_vectorizer.get_feature_names()[number]], end=\"\")\n",
    "        probability_list.append(number)\n",
    "    print(\"\\n\")\n",
    "    topic_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = best_lda_model.transform(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic selection\n",
    "Based on words, an arbitory topic name is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_desc_list = {0: \"Education\", \n",
    "                   1: \"Research\", \n",
    "                   2: \"Law\", \n",
    "                   3: \"Sport\", \n",
    "                   4: \"Finance\", \n",
    "                   5: \"Health\", \n",
    "                   6: \"horoscopes\", \n",
    "                   7: \"Environment\", \n",
    "                   8: \"Economy\", \n",
    "                   9: \"Various\", \n",
    "                   10: \"Sport\", \n",
    "              }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning the topic number with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = []\n",
    "# topics is a list of arrays containing \n",
    "# all index positions of words for each textfile\n",
    "for popular_index_pos in topics:\n",
    "    # Get the max index position in each array\n",
    "    # and add to the topic_list list\n",
    "    topic_list.append(popular_index_pos.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment of topic numbers to the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to the dataframe\n",
    "data_frame[\"Topic number\"] = topic_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using map function the topic descriptions are added to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_no_to_topic_desc = data_frame[\"Topic number\"].map(topic_desc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition the Topic Description column to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame[\"Topic desc\"] = topic_no_to_topic_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample dataframe with question, topic number and topic description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv(r'quora_supervised.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aim:\n",
    "\n",
    "\n",
    "#### Method:\n",
    "\n",
    "\n",
    "#### Expected output:\n",
    "\n",
    "--------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(\"quora_supervised.csv\", encoding='utf-8')\n",
    "data_frame.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_df is between 0-1 or an INT\n",
    "count_vectorizer = CountVectorizer(max_df=0.90, min_df=4, stop_words=\"english\")\n",
    "doc_term_matrix = count_vectorizer.fit_transform(data_frame[\"question\"])\n",
    "len(count_vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = count_vectorizer.fit_transform(data_frame[\"question\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_topic = data_frame['Topic number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(doc_term_matrix,target_topic, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier \n",
    "MultinomialNB classifier is created and used to fit/train the model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnc_classifier = MultinomialNB()\n",
    "mnc_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "The Model is now equipped with data to predict a new review is positive or negative. A **positive review** is fed into the classifier to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnc_model_predictions = mnc_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "In order to evaluate our model for the movie review classifer we are going to use Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(y_test, mnc_model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, mnc_model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
