Interactive Timeline Surgery 1812-2012.

Surgery is a profession defined by its authority to cure by means of bodily invasion. The brutality and risks of opening a living person's body have long been apparent, the benefits only slowly and haltingly worked out. Nonetheless, over the past two centuries, surgery has become radically more effective, and its violence substantially reduced — changes that have proved central to the development of mankind's abilities to heal the sick.

Surgery before the Advent of Anesthesia The first volume of the New England Journal of Medicine and Surgery, and the Collateral Branches of Science, published in 1812, gives a sense of the constraints faced by surgeons, and the mettle required of patients, in the era before anesthesia and antisepsis. In the April issue for that year, John Collins Warren, surgeon at the Massachusetts General Hospital and son of one of the founders of Harvard Medical School, published a case report describing a new approach to the treatment of cataracts.1 Until that time, the prevalent method of cataract treatment was “couching,” a procedure that involved inserting a curved needle into the orbit and using it to push the clouded lens back and out of the line of sight.2 Warren's patient had undergone six such attempts without lasting success and was now blind. Warren undertook a more radical and invasive procedure — actual removal of the left cataract. He described the operation, performed before the students of Harvard Medical School, as follows: The eye-lids were separated by the thumb and finger of the left hand, and then, a broad cornea knife was pushed through the cornea at the outer angle of the eye, till its point approached the opposite side of the cornea. The knife was then withdrawn, and the aqueous humour being discharged, was immediately followed by a protrusion of the iris. Into the collapsed orbit of this unanesthetized man, Warren inserted forceps he had made especially for the event. However, he encountered difficulties that necessitated improvisation: The opaque body eluding the grasp of the forceps, a fine hook was passed through the pupil, and fixed in the thickened capsule, which was immediately drawn out entire. This substance was quite firm, about half a line in thickness, a line in diameter, and had a pearly whiteness. A bandage was applied, instructions on cleansing the eye were given, and the gentleman was sent home. Two months later, Warren noted, inflammation required “two or three bleedings,” but “the patient is now well, and sees to distinguish every object with the left eye.” The implicit encouragement in Warren's article, and in others like it, was to be daring, even pitiless, in attacking problems of an anatomical nature. As the 18th-century surgeon William Hunter had told his students, “Anatomy is the Basis of Surgery, it informs the Head, guides the hand, and familiarizes the heart to a kind of necessary inhumanity.”3 That first volume of the Journal provided descriptions of a remarkable range of surgical techniques, including those for removing kidney, bladder, and urethral stones; dilating the male urethra when strictured by the passage of stones; tying off aneurysms of the iliac artery and infrarenal aorta; treating burns; and using leeches for bloodletting. There were articles on the problem of “the ulcerated uterus” and on the management of gunshot and cannonball wounds, not to mention a spirited debate on whether the wind of a passing cannonball alone was sufficient to cause serious soft-tissue injury. Surgery, nonetheless, remained a limited profession. Pain and the always looming problem of infection restricted the extent of a surgeon's reach. Entering the abdomen, for instance, was regarded with reproach — attempts had proved almost uniformly fatal.4 The chest and joints were also out of reach. The primary remit of surgery was therefore the management of external conditions, and medicine dealt with the internal ones (hence the term “internal medicine,” which persists to this day). Even for those conditions that appeared to be externally accessible, surgical accounts often spoke of failure more than derring-do. For example, in an article on spina bifida that appeared in the January 1812 issue of the Journal, a surgeon noted the uniform fatality of the condition and recounted an effort to repeatedly lance, drain, and bandage an infant's meningocele, which proved to be utterly futile.5 The skin “had become thickened, and as inelastic . . . as the upper leather of a shoe; it also ulcerated,” the author wrote. “Pus was formed in the sac, and the infant died.” Such reports often maintained an almost defiant optimism. (“We have no doubt,” this surgeon concluded, “that if performed with due caution,” a technique of draining meningoceles will be engineered and “the disease of Spina Bifida may cease to be an opprobrium of medicine.”) Nonetheless, breakthrough surgical successes were, for a long time, few and far between. They were also often illusory. In 1831, for instance, a Mr. Preston reported in the Journal his treatment of a man with an acute stroke that had resulted in left hemiparesis and speech difficulties.6 He did not use the usual, ineffective method of bloodletting and applying leeches but instead decided to take the curious approach of ligating the patient's right common carotid artery. Preston conjectured that by diminishing the supply of blood to the affected side of the brain, the treatment would reduce congestion and inflammation. By luck, the man survived. He was discharged 1 month later, walking with the aid of a stick and speaking normally, leading Preston to propose that surgeons might consider tying both carotids in future cases. Fortunately, his case notwithstanding, the procedure failed to catch on. The crucial spark of transformation — the moment that changed not just the future of surgery but of medicine as a whole — was the publication on November 18, 1846, of Henry Jacob Bigelow's groundbreaking report, “Insensibility during Surgical Operations Produced by Inhalation”7 (Figure 1Figure 1 Operation Being Performed with the Use of Ether Anesthesia.This daguerreotype was taken in the spring of 1847 by Josiah Hawes in the Operating Room (now known as the Ether Dome) of the Massachusetts General Hospital. The first public demonstration of surgical anesthesia occurred in the same room on October 16, 1846, presided over by the surgeon John Collins Warren, seen here touching the patient. Although it is believed that a photographer was present during the first event as well, he took no pictures because the sight of blood made him nauseated.8 Courtesy of the Massachusetts General Hospital, Archives and Special Collections. ). The opening sentences crisply summarized the achievement: “It has long been an important problem in medical science to devise some method of mitigating the pain of surgical operations. An efficient agent for this purpose has at length been discovered.” Bigelow described how William Morton, a Boston dentist, had administered to his own patients, and then to several more who had undergone surgery at the Massachusetts General Hospital, a gas he called “Letheon,” which successfully rendered them insensible to pain. Morton had patented the composition of the gas and kept it a secret even from the surgeons. Bigelow revealed, however, that he could smell ether in it. The news burst across the world. The Letters to the Editor pages were occupied for months with charges and countercharges over Bigelow's defense of Morton's secrecy and credit for the discovery. Meanwhile, ether anesthesia rapidly revolutionized surgery — how it was practiced, what could be attempted with its use, and even what it sounded like. Consider, for instance, amputation of the leg. The procedure had long been recognized as lifesaving, in particular for compound fractures and other wounds prone to sepsis, and at the same time horrific. Before the discovery of anesthesia, orderlies pinned the patient down while an assistant exerted pressure on the femoral artery or applied a tourniquet on the upper thigh (Figure 2A, upper drawingFigure 2 Methods of Amputation in the Early 19th Century.Panel A is a drawing by Charles Bell from 1821 showing the circular method of amputation.9 Panel B shows the flap method of amputation being used in 1837, with an assistant retracting the tissue flap to allow the surgeon to saw through the femur.10 ). Surgeons using the circular method proceeded through the limb in layers, taking a long curved knife in a circle through the skin first, then, a few inches higher up, through the muscle, and finally, with the assistant retracting the muscle to expose the bone a few inches higher still, taking an amputation saw smoothly through the bone so as not to leave splintered protrusions (Figure 2A, lower drawing). Surgeons using the flap method, popularized by the British surgeon Robert Liston, stabbed through the skin and muscle close to the bone and cut swiftly through at an oblique angle on one side so as to leave a flap covering the stump (Figure 2B). The limits of patients' tolerance for pain forced surgeons to choose slashing speed over precision. With either the flap method or the circular method, amputation could be accomplished in less than a minute, though the subsequent ligation of the severed blood vessels and suturing of the muscle and skin over the stump sometimes required 20 or 30 minutes when performed by less experienced surgeons.9 No matter how swiftly the amputation was performed, however, the suffering that patients experienced was terrible. Few were able to put it into words. Among those who did was Professor George Wilson. In 1843, he underwent a Syme amputation — ankle disarticulation — performed by the great surgeon James Syme himself. Four years later, when opponents of anesthetic agents attempted to dismiss them as “needless luxuries,” Wilson felt obliged to pen a description of his experience11: The horror of great darkness, and the sense of desertion by God and man, bordering close on despair, which swept through my mind and overwhelmed my heart, I can never forget, however gladly I would do so. During the operation, in spite of the pain it occasioned, my senses were preternaturally acute, as I have been told they generally are in patients in such circumstances. I still recall with unwelcome vividness the spreading out of the instruments: the twisting of the tourniquet: the first incision: the fingering of the sawed bone: the sponge pressed on the flap: the tying of the blood-vessels: the stitching of the skin: the bloody dismembered limb lying on the floor. Before anesthesia, the sounds of patients thrashing and screaming filled operating rooms. So, from the first use of surgical anesthesia, observers were struck by the stillness and silence. In London, Liston called ether anesthesia a “Yankee dodge” — having seen fads such as hypnotism come and go — but he tried it nonetheless, performing the first amputation with the use of anesthesia, in a 36-year-old butler with a septic knee, 2 months after the publication of Bigelow's report.10 As the historian Richard Hollingham recounts, from the case records, a rubber tube was connected to a flask of ether gas, and the patient was told to breathe through it for 2 or 3 minutes.12 He became motionless and quiet. Throughout the procedure, he did not make a sound or even grimace. “When are you going to begin?” asked the patient a few moments later. He had felt nothing. “This Yankee dodge beats mesmerism hollow,” Liston exclaimed. It would take a little while for surgeons to discover that the use of anesthesia allowed them time to be meticulous. Despite the advantages of anesthesia, Liston, like many other surgeons, proceeded in his usual lightning-quick and bloody way. Spectators in the operating-theater gallery would still get out their pocket watches to time him. The butler's operation, for instance, took an astonishing 25 seconds from incision to wound closure. (Liston operated so fast that he once accidentally amputated an assistant's fingers along with a patient's leg, according to Hollingham. The patient and the assistant both died of sepsis, and a spectator reportedly died of shock, resulting in the only known procedure with a 300% mortality.)

A New Era of Anesthesia and Antisepsis Surgeons soon found, however, that anesthesia allowed them to perform more complex, invasive, and precise maneuvers than they had dared to attempt before. Within a decade, for instance, the first successful hysterectomy and bilateral ovariotomy — removal of massive ovarian cysts weighing several pounds13,14 — proved that the abdomen could be safely penetrated. Further experiments revealed other effective anesthetics: nitrous oxide, chloroform, and eventually halothane and other nonvolatile agents. Narcotics such as laudanum were found to relieve postoperative suffering. Suddenly, pain was no longer a barrier to surgical capability. A second major barrier persisted, however: sepsis. The mortality associated with ovariotomy and other types of major abdominal surgery, repair of open fractures, and limb amputation commonly remained at 50% or higher owing to infection.15 One therefore might have thought that the news of Joseph Lister's landmark series of articles in The Lancet in 186716 describing the effectiveness of his new system of antisepsis with the use of carbolic acid would be received with the same fanfare as the report on ether anesthesia had been (Figure 3Figure 3 Introduction of Carbolic Acid for Antisepsis.In the late 19th century, Joseph Lister introduced antisepsis, using carbolic acid for hand washing, for dressings, and as a spray over the operative field to prevent infection in the surgical wound.8 Drawing by William Watson Cheyne, courtesy of Wellcome Library, London. ). Instead, it was regarded with overwhelming skepticism. The Journal first mentions Lister's breakthrough as a method that was neither original nor beneficial.17 Nearly a decade later, a surgeon writing in the Journal on the dressing of wounds could still insist “that there is good reason to believe that the theory of M. Pasteur, upon which Lister bases his treatment, is unsound.”18 Ignaz Semmelweiss, the Viennese obstetrician who in 1847 had found that hand washing by birth attendants eliminated puerperal sepsis, the leading cause of maternal death,19 was not even mentioned in the Journal until the end of the 19th century. J.M.T. Finney recalled his experience as a house officer at the Massachusetts General Hospital in the 1880s: “The operating surgeon was usually garbed in a black Prince Albert coat, kept hanging in a closet for the occasion and showing numerous evidences of previous operations in the way of dried blood, wound secretions, etc.”20 For decades, hand washing and skin cleansing remained routinely perfunctory. Some surgeons, however, especially younger ones, began accepting the diligence required for aseptic and antiseptic practice. Such practice, along with effective anesthesia, led them to hitherto unimagined treatments and discoveries. In 1868, for instance, John Stough Bobbs reported on a 30-year-old woman with a large, painful right abdominal mass presumed to be an ovarian cyst.21 Chloroform allowed him to explore her abdomen through a 4-in. incision. Sweeping the omental adhesions aside with a finger, he encountered a 5-in.–wide, smooth-walled, oval tumor. When he cut through the wall of the tumor, “a perfectly limpid fluid escaped, propelling, with considerable force, several solid bodies about the size of ordinary rifle bullets.” He drained the sac of its fluid, extracted some 50 concretions that ranged in size “from that of a mustard seed to that of a bullet,” and then closed the incision, uncertain what the concretions were. The patient recovered uneventfully with the help of laudanum and lemonade. Only later, when he carefully examined the smooth, mahogany-colored, irregularly spherical objects that he had extracted, did Bobbs understand what he had encountered. They were gallstones. The absence of green–yellow bile in the sac had confused him — the clear, mucoid fluid would come to be known as “white bile” — but he had, in fact, performed the world's first successful gallbladder operation. A slew of “firsts” rapidly followed, each more daring than the last. In 1880, Tait performed the first transabdominal resection of a gangrenous appendix,22 and Rehn performed the first subtotal thyroidectomy for Graves' disease.23 In 1884, Bennett and Godlee reported the first successful removal of a brain tumor.24 Methods for suprapubic prostatectomy, total gastrectomy, chest surgery, and joint repair were worked out.15,25 Alexis Carrel devised methods for suturing blood vessels and performing surgical grafts that became the foundation for the field of vascular surgery and won him a Nobel Prize in 1912.26 Surgeons developed such skill and confidence that they began performing exploratory laparotomies simply for the purpose of diagnosis. Indeed, articles raising concern that there were perhaps too many laparotomies began to appear by the turn of the century.27 The key barriers to surgical knowledge and imagination were gone. Until this time, surgical discoveries had provided only halting contributions to the capabilities of the medical community. In the early part of the 19th century, just one fifth of the Journal's scientific articles were surgical in nature (if one takes as a guide the review and classification of each article in the first volume for each decade, beginning with 1812) (Figure 4Figure 4 Changes in the Proportion of Articles on Surgery Published in the Journal since 1812.The percentages shown are based on a review of the scientific articles in the first volume of issues published by the Journal for each decade since 1812. ). Surgery had been, one might politely say, a modest contributor to medical progress. Between the mid-1800s and the 1920s, however, the coverage of surgical advances took up half the Journal. Physicians in the Victorian era had few effective drugs, but surgeons began reporting new treatments almost monthly, and the breakneck pace of innovation continued for nearly a century. Surgery became a dominant force in medical advancement.

Professionalization, Minimization, and Routinization Surgery also began to progress through an increasingly important process of refinement and professionalization. William Halsted introduced and popularized the use of rubber gloves to help prevent infection. Care of burns and other wounds was made radically simpler and less traumatizing. Anesthetic techniques and apparatus were being made more reliable and sophisticated. And in 1917, the American College of Surgeons founded the Hospital Standardization Program (later renamed the Joint Commission for the Accreditation of Hospitals) to shift the role of hospitals from serving primarily as a place for the convalescence of the sick poor to providing safe and effective care for patients undergoing surgery. Specialization was likewise an important force. Historians continue to debate whether the growth in knowledge drove specialization or specialization led to the growth in knowledge. (There are numerous examples of each.) Nonetheless, in 1905, the Long Island Society of Anesthetists was formed, which would evolve into the American Society of Anesthesiologists.28 After World War I, national associations were formed for neurologic surgeons, orthopedic surgeons, urologists, and other specialists, and dedicated training programs were established. Surgery — the invasion of people's bodies for cure — was becoming normalized. Much of the story appeared only obliquely in the Journal. But that too reflected the changing nature of progress. The milestone reports of the new era often seemed obscure when they first appeared. Werner Forssmann, a 25-year-old surgical intern in Eberswalde, Germany, published his report on successful catheterization of the heart in a German medical journal in 1929.29 Forbidden by his professors from attempting the experiment on either animals or patients (they thought the idea preposterous and dangerous), he performed it on himself (Figure 5Figure 5 Radiograph of the Heart Self-Catheterized by a Surgical Intern in 1929.The radiograph shows successful self-catheterization of the heart, performed by Werner Forssmann, at the time a 25-year-old surgical intern in Eberswalde, Germany.29 ). His investigation would eventually lead to the creation of the field of cardiology and win Forssmann the Nobel Prize in Medicine. Yet it was more than a decade before anyone recognized the significance of his report. Likewise, anesthesia resident David Massa's ingenious creation of the plastic intravenous catheter probably seemed a minor innovation to many at the time. A description of his device appeared in 1950 in the Proceedings of the Staff Meetings of the Mayo Clinic under the modest title “A Plastic Needle,” and it was not until the 1960s that this type of catheter became well known.30 Eventually, however, it transformed the approach to patient resuscitation. The field of surgery, with its ethos of radical action and perfectionist refinement, defined much of medical culture in the early 20th century. By midcentury, however, surgery's outsize role and influence began to subside. Whereas its discoveries had taken up half the space in the Journal in 1922, the proportion fell to one third during the next decade. By the 1950s, reports of new diagnostic tests, vaccines, antibiotics, and other innovations from the wet laboratory dominated the pages of the Journal. Scientists had found an even more prolific source of discovery than the operating room: the laboratory bench. With the advent of chemotherapeutics, molecular medicine, and other technologies, surgery was no longer the driving force behind medical breakthroughs. Since 1972, just a tenth of the Journal's articles have been devoted to surgical advances. To be sure, the field of surgery continued to register a steady stream of seminal breakthroughs. This was the era in which the heart was conquered, after all. In 1948, Dwight Harken and colleagues published an astonishing report describing the successful surgical treatment of mitral-valve disease31; in 1945, Blalock and Taussig designed their shunt operation for “blue babies”32; Robert Gross and colleagues reported in 1952 on open-heart surgery to close atrial septal defects in children33; and the development of cardiopulmonary-bypass technology allowed open-heart procedures to be carried out. Similarly, transplantation of organs between human beings — first kidneys, then livers, then hearts and lungs, and most recently, even faces — altered basic concepts about ourselves and led us to redefine death. Implantation of organs engineered in the laboratory from a person's own cells is now being reported.34 Surgeons are still traversing remarkable frontiers. But the most striking story of surgery in recent decades is how firmly it has become established as an essential tool for helping people live long and healthy lives. Virtually no one escapes having a condition for which effective treatment requires surgery — a serious orthopedic injury, a cataract, a tumor, obstructed labor, joint failure, severe cardiac disease. Today, surgeons have in their arsenal more than 2500 different procedures. Thus, the focus of recent advances in the field has been less on adding to the arsenal than on ensuring the successes of the treatments we have. Minimization of the invasiveness of surgical procedures is an advance that is arguably as significant as the discovery of anesthesia. In recent decades, the advent of laparoscopy and thoracoscopy reduced the debilitating, half-meter-long abdominal and chest incisions to a half centimeter. The subsequent introduction of endoscopic and percutaneous techniques has turned incisions into mere puncture wounds. Gallbladder surgery, resection of colonic polyps and ovarian tumors, and lung biopsies have become outpatient procedures. We are now in an era in which a teenage boy can undergo reoperation for repair of a severe coarctation of his thoracic aorta percutaneously on a Thursday and be well enough to sprain his ankle playing sports the following Saturday (as my son did not long ago). The technological refinement of our abilities to manipulate the human body has been nothing short of miraculous. The increased safety and ease of surgery have produced an explosion in the volume of operations being performed — to at least 50 million annually in the United States alone. At the present rate, the average American can expect to undergo seven operations during his or her lifetime.35 This profound evolution has brought new societal concerns, including how to ensure the quality and appropriateness of the procedures performed, how to make certain that patients have access to needed surgical care nationally and internationally, and how to manage the immense costs. As early as the 1970s, researchers began documenting substantial rates of fatal errors in surgical care, wide differences in outcomes among institutions, and large disparities in access to care both within the United States and between countries. The science of effectively routinizing surgery for mass populations is still in its infancy, as it is for all areas of medicine. The Journal is entering its third century of publication, yet we are still unsure how to measure surgical care and its results. Experiments in the delivery of care will probably provide the next major advancement in the field of surgery. Meanwhile, the practice of surgery itself will continue to change. Prognostication is a hazardous enterprise. But if the past quarter century has brought minimally invasive procedures, the next may bring the elimination of invasion. One feels foolish using terms like nanotechnology — I haven't the slightest idea what it really means or can do — but scientists are already experimenting with techniques for combining noninvasive ways of seeing into the body through the manipulation of small-scale devices that can be injected or swallowed. Surgical work will probably even become fully automated. The possibilities are tantalizing. A century into the future, a surgeon will tell the tale — that is, if the world still makes such people.

Disclosure forms provided by the author are available with the full text of this article at NEJM.org. This article (10.1056/NEJMra1202392) was updated on August 9, 2012, at NEJM.org. I thank Ami Karlage for assistance with the historical research.