One main concern for people in the tech industry would be if regulators jumped in to create rules around their A.I. work. So they are trying to create a framework for a self-policing organization, though it is not clear yet how that will function.

“We’re not saying that there should be no regulation,” said Peter Stone, a computer scientist at the University of Texas at Austin and one of the authors of the Stanford report. “We’re saying that there is a right way and a wrong way.”

While the tech industry is known for being competitive, there have been instances when companies have worked together when it was in their best interests. In the 1990s, for example, tech companies agreed on a standard method for encrypting e-commerce transactions, laying the groundwork for two decades of growth in internet business.

The authors of the Stanford report, which is titled “Artificial Intelligence and Life in 2030,” argue that it will be impossible to regulate A.I. “The study panel’s consensus is that attempts to regulate A.I. in general would be misguided, since there is no clear definition of A.I. (it isn’t any one thing), and the risks and considerations are very different in different domains,” the report says.

Photo

One recommendation in the report is to raise the awareness of and expertise about artificial intelligence at all levels of government, Dr. Stone said. It also calls for increased public and private spending on A.I.

“There is a role for government and we respect that,” said David Kenny, general manager for IBM’s Watson artificial intelligence division. The challenge, he said, is “a lot of times policies lag the technologies.”

A memorandum is being circulated among the five companies with a tentative plan to announce the new organization in the middle of September. One of the unresolved issues is that Google DeepMind, an Alphabet subsidiary, has asked to participate separately, according to a person involved in the negotiations.

Newsletter Sign Up Continue reading the main story Please verify you're not a robot by clicking the box. Invalid email address. Please re-enter. You must select a newsletter to subscribe to. Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services. Thank you for subscribing. An error has occurred. Please try again later. View all New York Times newsletters.

The A.I. industry group is modeled on a similar human rights effort known as the Global Network Initiative, in which corporations and nongovernmental organizations are focused on freedom of expression and privacy rights, according to someone briefed by the industry organizers but not authorized to speak about it publicly.

Advertisement Continue reading the main story

Separately, Reid Hoffman, a founder of LinkedIn who has a background in artificial intelligence, is in discussions with the Massachusetts Institute of Technology Media Lab to fund a project exploring the social and economic effects of artificial intelligence.

Both the M.I.T. effort and the industry partnership are trying to link technology advances more closely to social and economic policy issues. The M.I.T. group has been discussing the idea of designing new A.I. and robotic systems with “society in the loop.”

The phrase is a reference to the long-running debate about designing computer and robotic systems that still require interaction with humans. For example, the Pentagon has recently begun articulating a military strategy that calls for using A.I. in which humans continue to control killing decisions, rather than delegating that responsibility to machines.

“The key thing that I would point out is computer scientists have not been good at interacting with the social scientists and the philosophers,” said Joichi Ito, the director of the MIT Media Lab and a member of the board of directors of The New York Times. “What we want to do is support and reinforce the social scientists who are doing research which will play a role in setting policies.”

The Stanford report attempts to define the issues that citizens of a typical North American city will face in computers and robotic systems that mimic human capabilities. The authors explore eight aspects of modern life, including health care, education, entertainment and employment, but specifically do not look at the issue of warfare. They said that military A.I. applications were outside their current scope and expertise, but they did not rule out focusing on weapons in the future.

The report also does not consider the belief of some computer specialists about the possibility of a “singularity” that might lead to machines that are more intelligent and possibly threaten humans.

“It was a conscious decision not to give credence to this in the report,” Dr. Stone said.