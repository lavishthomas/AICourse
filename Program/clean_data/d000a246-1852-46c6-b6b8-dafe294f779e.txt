As fate would have it, I had just shown my students at Williams College the grainy footage of Burden’s shooting when we learned of his death in May. Curiously, the clip did not provoke them as it had their predecessors in my classrooms in decades past. No one expressed any palpable sense of shock or revulsion, let alone the idea that the proper response to the violation of a taboo is honest outrage. One student fretted about the legal liability of the shooter; another intelligently placed the work in historical context and related it to anxiety over the Vietnam War.

Placing things in context is what contemporary students do best. What they do not do is judge. Instead there was the same frozen polite reserve one observes in the faces of those attending an unfamiliar religious service—the expression that says, I have no say in this. This refusal to judge or take offense can be taken as a positive sign, suggesting tolerance and broadmindedness.

But there is a broadmindedness so roomy that it is indistinguishable from indifference, and it is lethal. For while the fine arts can survive a hostile or ignorant public, or even a fanatically prudish one, they cannot long survive an indifferent one. And that is the nature of the present Western response to art, visual and otherwise: indifference.

In terms of quantifiable data—prices spent on paintings and photographs and sculptures, visitors accommodated and funds raised and square footage created at museums—the picture could hardly be rosier. On May 1, New York’s Whitney Museum moved from 75th Street to the Meatpacking District and reopened in a $422 million building. The move first seemed inexplicable when it was announced several years ago, but it has proved to be a brilliant stroke. Relocating to a hotter and more fashionable neighborhood abutting the wildly popular High Line Park is one of those gambits that instantly transforms the logic of the game board, like castling in chess. The new Whitney was designed by the furiously prolific Renzo Piano, and while it will not please everyone (its boxy swivel of platforms distressingly recalls the flight tower of an aircraft carrier), it swaggers with brio and panache and is fast becoming one of the world’s best-attended museums.

Equally robust is the art market, to judge by a Christie’s auction on May 11 that set several records, including the highest price ever paid at auction for a work of art: $179.4 million, paid by an anonymous bidder, for Picasso’s Women of Algiers (Version O). One can expect more such record-breaking in the next few years as the art market is increasingly roiled by Hong Kong dollars, Swiss francs, and Qatari riyals. (The buyer of the Picasso was subsequently revealed to be the former prime minister of Qatar.)

But quantifiable data can only describe the fiscal health of the fine arts, not their cultural health. Here the picture is not so rosy. A basic familiarity with the ideas of the leading artists and architects is no longer part of the essential cultural equipment of an informed citizen. Fifty years ago, educated people could be expected to identify the likes of Saul Bellow, Buckminster Fuller, and Jackson Pollock. Today one is expected to know about the human genome and the debate over global warming, but nobody is thought ignorant for being unable to identify the architect of the Freedom Tower or name a single winner of the Tate Prize (let alone remember the name of the most recent winner of the Nobel Prize in Literature).

The last time that artists were part of the national conversation was a generation ago, in 1990. This was the year of the NEA Four, artists whose grants were withdrawn by the National Endowment for the Arts because of the obscene content of their work. Their names were Tim Miller, John Fleck, Holly Hughes, and Karen Finley—the latter especially famous because her most notable work largely involved smearing her own body with chocolate. As it happened, their work was rather less offensive than that of Andres Serrano and Robert Mapplethorpe, who had been the subject of NEA-funded exhibitions the year before. Serrano’s photograph of a crucifix immersed in a jar of his own urine was called “Piss Christ.” Mapplethorpe’s notorious self-portrait featured a bullwhip thrust into his fundamental aperture. Even the New York Times, a stalwart champion of Mapplethorpe, could not honestly describe that photograph, let alone publish it, referring to it with coy primness as a “sadomasochistic self-portrait (nearly naked, with bullwhip).”

That controversy ended with a double defeat. In a case that was heard by the Supreme Court, the NEA Four failed to have their grants restored. But Senator Jesse Helms and Representative Newt Gingrich likewise failed in their determined effort to defund the NEA (total budget at the time: $165 million). And the American public—left with an impressionistic vision in which urine, bullwhips, and a naked but chocolate-streaked Karen Finley figured largely—drew the fatal conclusion that contemporary art had nothing to offer them. Fatal, because the moment the public disengages itself collectively from art, even to refrain from criticizing it, art becomes irrelevant.

This essay proposes that such a disengagement has already taken place, and that its consequences are dire. The fine arts and the performing arts have indeed ceased to matter in Western culture, other than in honorific or pecuniary terms, and they no longer shape in meaningful ways our image of ourselves or define our collective values. This collapse in the prestige and consequence of art is the central cultural phenomenon of our day.

It began a century ago.

For most of human history, works of visual art were the direct expression of the society that made them. The artist was not an autonomous creator; he worked at the behest of his patron, making objects that expressed in visible form that patron’s beliefs and aspirations. As society changed, its chief patrons changed—from medieval bishop to absolutist despot to captain of industry—and art changed along with it. Such is patronage, the mechanism by which the hopes, values, and fears of a society make themselves visible in art. When World War I broke out in 1914, that mechanism was delivered a blow from which it never quite recovered. If human experience is the raw material of art, here was material aplenty but of the sort that few patrons would choose to look upon.

When I went to Germany to study architecture in 1980, it was still common to see wounded veterans from both world wars. The first seats on buses or subways were reserved for them and were usually occupied. One day a fellow student and fellow history buff chanced to remark that the worst cases were from World War I, but of course one never saw them.

“They never show their faces?” I asked, naively.

“Mike,” he said, “they have no faces to show.”

He was speaking of the Verstümmelten—the mutilated, those who lost jaws, cheeks, and noses in the shambles of the trenches, where a raised head was the target of choice. They had been so wounded, brought to a nearby field lazaretto and put together as much as possible, that they would spend the rest of their lives seen only by the family members who tended to them. “There is one,” my friend said—I can still picture the expansive gesture he made—“in practically every street.”

Not until then did I properly grasp the unbearable intensity of German artists such as Ernst Kirchner, Otto Dix, and George Grosz, who created their most memorable work during the war or just afterward. The human body—dynamic, beautiful, created in God’s image—had long been the central subject of Western art. It was now depicted in the most tormented and fragmented manner, every coil of innards laid bare with obscenely morbid imagination. Kirchner and Dix depicted the gore. Grosz, who refrained from showing actual injuries, was even more disturbing. He made collages of faces out of awkwardly assembled parts, like a jigsaw puzzle assembled with the wrong pieces, suggesting those sad prosthetics that would have been an ubiquitous presence in 1918.

Christianity had introduced the motif of beautiful suffering, in which even the most agonizing of deaths could be shown to have a tragic dignity. But things had now been done to the human body that were unprecedented, and on an unprecedented scale. The cruel savagery of this art can be understood only as the product of collective trauma, like the babble of absurd free associations that tumble from our mouths when in a state of shock. That kind of irrational expression was the guiding principle of Dada, the movement that came about at the end of the war and that was made famous by Marcel Duchamp’s celebrated urinal turned upside down and named Fountain. Even the name Dada itself was a quintessential absurdist performance, selected at random from a French-German dictionary (the word is French baby talk for “hobby-horse”).

Dada applied unserious means to a serious end: the search for an artistic language capable of expressing the monstrous scale and nature of the war. But the absurdist moment was short-lived and quickly superseded. The toppling of Europe’s three principal empires and the Russian Revolution seemed to confirm that the West had entered into a radical new phase of cultural history, the most consequential since the rise of Renaissance humanism half a millennium ago. There was a general sense that a world radically transformed by war required an equally radical new art—an art of urgent gravity. While modern art had certainly existed before the war, there now came into being a comprehensive “modern movement” that was active in all spheres of human action, not only in art but in politics and science as well. In its wake, Pablo Picasso rose from being a mere painter with a quirky personal style to a world-historical figure whose work was as important to the future of mankind as Einstein’s or Freud’s.

All this gave the modernism of the 1920s its tone of moral seriousness, which became even more serious once the Great Depression began. One sees this high-minded seriousness most strikingly in the architects of the modern movement. They saw their mandate as the solving of the central architectural challenge of modern life—how to use new materials and means of construction to make housing affordable and cities humane. Today the lofty principles that motivated them seem quaint, such as that German fixation on the Existenzminimum. This was the notion that in the design of housing, one must first precisely calculate the absolute minimum of necessary space (the acceptable clearance between sink and stove, between bed and dresser, etc.), derive a floor plan from those calculations, and then build as many units as possible. One could not add a single inch of grace room, for once that inch was multiplied through a thousand apartments, a family would be deprived of a decent dwelling. So went the moral logic.

We may shudder at the thought of so many identical families penned into so many identical boxes, but at root this idea was an expression of a humane and humanistic view of the world. It took for granted that the mission of architecture was to improve the human condition, even as modern artists assumed that theirs was to express that condition. To accomplish this, they did not require the traditional patron. The prestige and power of those patrons had been diminished by the war, and with that diminution went their ability to dictate to artists. That became even more pronounced after the stock-market collapse in the late 1920s, especially in the United States. When the Museum of Modern Art introduced European modern architecture to America in its celebrated 1932 exhibition of the International Style, it dismissed the traditional capitalist client with remarkable highhandedness. A half century of robust artistic and architectural patronage by the industrialists who had ruled American life since the Gilded Age was written off with a sneer by the exhibition’s organizer, Alfred Barr: “We are asked to take seriously the architectural taste of real-estate speculators, renting agents, and mortgage brokers.” In other words, the making of art was far too serious to be left to sentimental clients who might mistakenly desire a narrative painting with a clear moral message, or a facsimile of a villa they had admired in Tuscany.

After World War II and the introduction of the atom bomb, it seemed pointless to try to preserve the confused traditions of a civilization that had brought the world to the ledge of oblivion. Instead, the artists came to believe they had to dispense with the entire accumulated storehouse of artistic memory and the history of the benighted West in order to begin anew.

The 1950s painter Barnett Newman summarized this line of thought pretentiously but accurately:

We are freeing ourselves of the impediments of memory, association, nostalgia, legend, myth, or what have you, that have been the devices of western European painting. Instead of making “cathedrals” out of Christ, man, or “life,” we are making it out of ourselves, out of our own feelings.

“We are making it out of ourselves” is a fair summary of the revolution in patronage the modern movement had brought about, in which the artist himself had now been transformed into his own patron. And yet, radical as were Newman’s existentialist “zip” paintings, consisting of spare, massive vertical stripes smoldering like a cosmic portal about to open, they remained traditional in one key respect: They still existed in a recognizable moral universe. For all their portentous grandiloquence, the zip paintings still speak of that ancient durable strand in the Western tradition, a belief in the tragic dignity of man.

With the irreversible decline of that belief, it may be said that the age of postmodernism began, and that the journey to the present state of monumental public indifference to art started to accelerate.

Susan Sontag’s 1964 essay “Notes on ‘Camp’” is the first attempt to define the rapid change in attitude that was taking place in the early 1960s toward art, society, tradition, everything. It was not so much a change in style or philosophy as in sensibility. This new sensibility, Sontag wrote, “sees everything in quotation marks” and “converts the serious into the frivolous.” Although the condition of the world seemed ever more serious—the Cuban Missile Crisis had just taken place—a younger generation in the Western democracies had determined that the proper response was to be even less serious, to throw up one’s hands and confront the world with irony. Even as Sontag wrote, that new sensibility was being reflected in painting (Andy Warhol), sculpture (Claes Oldenburg), and architecture (Robert Venturi), each of whose works exist, in some sense, in quotation marks. Common to all was a shared posture of irreverence and ironic detachment. This new dispensation worked itself through the veins and arteries of the art world with remarkable briskness from studio to gallery to museum to classroom. In one respect, the insouciance of pop art came as a refreshing breeze. Warhol’s soup cans and Oldenburg’s giant floppy hamburgers made no momentous claims to have banished nostalgia, legend, and myth; though in part they were intended to reveal the hollowness of capitalist commercial culture, they could not help but offer subject matter that was attractive, reassuring, and familiar. These were the still lifes of prosperous postwar America, and, like their 19th-century counterparts with their glistening fruit and vegetables, they spoke of abundance.

Only in architecture did the new sensibility take a while to establish itself. What client would wish to invest their millions in an “ironic statement”? The Quaker sponsors of Guild House, a retirement home in Philadelphia, were chagrined to learn that the metal sculpture over the entrance was meant to represent a giant television antenna—in the words of its architect Robert Venturi, “a symbol of the aged, who spend so much time looking at TV.” Unimpressed by his claim that it was inspired by the Château d’Anet, they promptly had it removed.

In the same way that pop art was meant as criticism yet featured a welcome note of playfulness, postmodern architecture did offer a playful, witty alternative to a modernism grown stale and formulaic. The calamitous failure of urban renewal was now everywhere apparent, and modernism’s claim that it could create humane cities was exploded definitively in Jane Jacobs’s brilliant The Death and Life of Great American Cities (1961). After a time, postmodernism’s jaunty irony was carried into the American cityscape by commercial clients such as AT&T who built the first uninhibitedly witty skyscrapers since the art deco towers of the Roaring Twenties. The burden art had carried since the end of World War I—the obligation to express ponderous things in ponderous ways, the burden to be on perpetual guard duty in the avant-garde, ever alert to any reactionary tendency—had been cast off.

For the most part, though, this new nonchalance was short-lived. Oldenburg’s goofy seven-foot Floor Burger (1962) was pleasantly apolitical. But a mere seven years later, he would produce the most memorable item of antiwar art to come out of the Vietnam War, Lipstick (Ascending) on Caterpillar Tracks, a parody of a tank he erected in front of Yale University’s administration building. Instead of a gun turret above its treads, it sported an oversized lipstick, not proudly erect but distressingly flaccid (Oldenburg’s sardonic commentary on masculinity and war). Although it was removed within a year (to be replaced later by an inferior replica in permanent materials), Lipstick was a harbinger of the return of seriousness to the art scene, a seriousness now tinged with fury, indignation, and, increasingly, politics.

A whole spectrum of other political causes soon found expression in art—environmentalism, feminism, Chicano rights. This new seriousness differed sharply from the old. If modernism had understood itself to be upholding and developing the culture from within, revolutionizing Western art in order to save it, its postmodern successors offered a critique from without. This was the counterculture that emerged after the collapse of the postwar liberal consensus, and its stance was essentially adversarial, distinguished by hostility to the existing order. It viewed the advanced industrial society of the West not as the highest development of human civilization but rather as a corrupt enterprise whose shameful legacy was slavery, colonialism, and exploitation.

It is easy to see why an artistic culture unwilling to champion even the abstract concept of Western culture would feel resentful toward a modernism that sought to do just that and would try to cut it down to size. Indeed, a canard was widely disseminated that abstract expressionism—the least political of all art movements in its rejection of the politicized social realism of the 1930s, which found its most enduring expression in Jackson Pollock’s drip paintings—was itself a CIA plot. According to this deranged view, the Congress for Cultural Freedom had sponsored international exhibitions of artists such as Pollock and Willem de Kooning as part of a CIA-subsidized campaign of cultural imperialism. While this highly tendentious reading of history has been roundly discredited, it has passed into the conventional professional wisdom.

Most of this slipped under the radar of the American public, which had by the 1970s established a kind of concordat with the art world. Whatever art had to offer—minimalism, conceptualism, photorealism—was a zany precinct where anything might happen, a source of entertainment, a zone that might be safely regarded with benign neglect. There was no arguing with success: When confronted with something as willfully bizarre as one of Robert Rauschenberg’s Combines (wherein a stuffed Angora goat might be inserted through an automobile tire), the public was happy to say “don’t ask, don’t tell” and tiptoe away with a smile on its collective face. After all, Rauschenberg was very famous, and his art made him very rich. If you didn’t like it, fine; somebody would and would pay huge sums for it.

Such was the concordat that fell apart spectacularly in the late 1980s, and when it did, artists were just as shocked as the public.

From time to time, so-called conceptual artists had looked to find new ways to use the human body artistically. Their agenda was by no means to express humanist values or even beautiful suffering—quite the contrary. In 1961, Piero Manzoni offered for sale 90 tin cans purportedly containing the Merda d’artista (to this day it is uncertain whether or not the cans actually contain his excrement, since to open one would cost on the order of $100,000). Manzoni’s foray into scatology was a prophecy of things to come. Ten years later, Vito Acconci became a minor celebrity with his performance of Seedbed, which involved his hiding under a platform in a gallery and speaking to visitors above while masturbating.

If these acts had any political agenda at all, it was anarchy. But in the wake of Roe v. Wade and then the AIDS epidemic, the human body assumed a political significance it had not had since abstract expressionism had banished life-drawing as one of those “devices of Western European art” that Barnett Newman had condemned. Now, as Barbara Kruger proclaimed in the celebrated poster she designed for a 1989 abortion-rights rally, “Your body is a battleground.”

There poured forth a great deal of body-centered art. Its one great constant was a high quotient of rage—as furious as any statue-smashing interlude in the long history of iconoclasm. Here was an anguish and loathing not seen since the days of Grosz and Dix, both of the self-hating variety (expressed through masochistic acts) and generalized rage against society (Serrano’s urine-immersed crucifix). I think of Ron Athey’s now notorious Four Scenes from a Harsh Life, for which he incised patterns into the back of a collaborator with a scalpel, dabbing up the blood with paper towels that were affixed to a clothesline and swung out over the wincing audience.

Such art, unlike that of Grosz, offered no coordinates from which society could navigate to find a higher purpose. Rather, it fulfilled the definition of what the late Philip Rieff called a “deathwork,” a work of art that poses “an all-out assault upon something vital to the established culture.”

Given this art’s flagrantly, deliberately transgressive nature, it is remarkable how surprised and bewildered its creators were when they felt the full measure of public disapproval, which came to a climax with the effort to defund the National Endowment for the Arts. After all, having been properly vetted and feted at every step by curators and journalists, academics and bureaucrats, these artists quite reasonably assumed that they were beyond reproach. That there was yet another actor out there in the mists, a public upon whose judgment their fate might depend—a public that might act to withdraw state funding of projects that were expressly intended to transgress its values—seems not to have crossed their minds.

One Harvard scholar suggested that Serrano erred because while he knew “his photograph to be provocative, he did not count on such a broad audience outside the art world.” But what to make of an artist who does not wish to have a broad audience or speak to his own society? At a minimum, it is not even political art—art that seeks to persuade or focus attention—if it exists only within the silo of its own echo chamber.

Although the body-art movement lost its incandescent fury as the AIDS crisis subsided, there lingered a fascination with the degraded human body. This reconfigured itself in the 1990s as the movement known as “abject art,” which the website of London’s Tate Gallery tactfully defines as “artworks which explore themes that transgress and threaten our sense of cleanliness and propriety particularly referencing the body and bodily functions.” The most notorious example came seven years ago when a Yale art student presented a performance of “repeated self-induced miscarriages.” According to her own account, she inseminated herself with sperm from voluntary donors, “from the 9th to the 15th day of my menstrual cycle…so as to insure the possibility of fertilization,” afterwards using “an herbal abortifacient” to induce the desired miscarriage. Here was indeed a deathwork, proud and unashamed. (The only solace is that she might not, in the end, have actually carried out her project in reality.)

Such projects, real or imaginary, returned the spotlight to the human body. But this was hardly the body that was, as Hamlet put it, “like a god in apprehension.” Rather, it was a ravaged and wounded thing, degraded and defenseless. One can almost understand the popularity of the ghastly flayed and “plastinated” bodies exhibited by Günther von Hagen, the notorious corpse artist, in his traveling “Body Worlds” exhibition.1 Unlike the degraded victimhood on display in most examples of abject art, his figures evoked dynamic action and freedom, and at least a shard of hope.

When viewed against this dispiriting backdrop, poor punctured and perforated Chris Burden seems like an Old Master.

And yet, and yet. Even as the public was flinching from the excesses of performance art and abject art, it was embracing museums as never before. The newly opened Whitney is the last of New York’s four major museums to renovate, enlarge, or replace its home in recent years. It reflects a worldwide trend that began in 1997 with the near simultaneous openings of the Getty Museum in Los Angeles and the Guggenheim Museum in Bilbao, Spain, which made overnight celebrities of their architects (Richard Meier and Frank Gehry, respectively).

Bilbao had another, more consequential effect. In just three years, it famously recouped the cost of its construction (which was underwritten by the provincial Basque government) partially through admissions but mostly through increased tourist revenue. This was dubbed “the Bilbao effect,” the notion that the building of a prestige museum can transform

the international profile of a city and make it a pilgrimage destination.

This helped launch a feverish wave of international museum-building that still shows no sign of abating. In fact, the only thing remotely like it is the cathedral-building boom of the Isle de France in the 13th century, when each city vied to build the loftiest, thinnest, and brightest Gothic nave.

If one compares the performance of museums to other entertainment facilities in the United States in terms of box office, the museums come off splendidly. According to the American Association of Museums, annual attendance hovers at about 700 or 800 million, and it did not even suffer declines during the recession of 2008. These figures far exceed the combined attendance at major-league sporting events and amusement parks. This is not by accident, for museums have been assiduously cultivating their attendance for quite some time. The process began with the “Treasures of Tutankhamen” exhibition that opened at the Metropolitan Museum of Art in 1978 and drew a record 1.8 million visitors. Startled museum trustees, previously accustomed to covering the annual deficit with a discreet check, took notice of the lines stretching around the block. The temptation proved irresistible, and the culture of the museum reoriented itself toward the regular production of a reliable blockbuster.

By any measure, there is hardly an institution in the Western world so healthy as the museum today. By any measure—there’s the rub. For some things cannot be measured but are important nevertheless. In 1998, exactly 20 years after the Tutankhamen exhibition, the Guggenheim brought forth “The Art of the Motorcycle,” an exhibition widely panned as without educational merit. Yet it, too, was a crowd-pleasing sensation, and it, too, broke attendance records. There may be a considerable difference between the gold mask of King Tut and a Harley-Davidson Sportster motorcycle, but not in the calculus of quantifiable data by which museums gauge their success. Still, it did not seem to trouble the general public that art museums now sported motorcycles and helicopters (in the lobby of the Museum of Modern Art), for they no longer expected museums to offer objects but an experience. The temple of the arts had been transformed into what the critic Jerry Saltz has called “a revved-up showcase of the new, the now, the next, an always-activated market of events and experiences, many of which lack any reason to exist other than to occupy the museum industry.”

Nor was the academy willing to draw any helpful distinction between King Tut and the Harley-Davidson, certainly not on the basis of aesthetic quality. To claim that the mask of Tut was in some way superior— to privilege it, in academic jargon—required making aesthetic or historical determinations that scholars were increasingly unwilling to make. For a generation they had been schooled by those French 20th-century intellectuals Michel Foucault and Jacques Derrida who said that art and literature were best understood as expressions of a structure of power. Better to eliminate altogether the word art, which evokes unhappy images of dominant cultures expressing their hegemony, in favor of the aesthetically neutral term visual culture, which makes no judgments about merit but merely looks at the purposes for which one makes works of art—or, in their terms, “objects of visual interest.” Thus with no clear directive from society to stress the good, museums have by default chosen to stress the new. The Met’s most successful exhibit over the past 10 years was a display of high-couture dresses by Alexander McQueen, a fashion designer who had hanged himself in his London apartment a year earlier.

Surely, such a large category of human achievement as the fine arts cannot simply become irrelevant and cease to matter. Or can it? In his prophetic essay “The Prevention of Literature,” George Orwell speculated that literature might one day be crippled by totalitarian regimes’ enforcement of intellectual conformity. The writing of imaginative prose demands an unfettered intellect, something incompatible with political orthodoxy of any stripe:

Prose literature as we know it is the product of rationalism, of the Protestant centuries, of the autonomous individual. And the destruction of intellectual liberty cripples the journalist, the sociological writer, the historian, the novelist, the critic, and the poet, in that order. In the future it is possible that a new kind of literature, not involving individual feeling or truthful observation, may arise, but no such thing is at present imaginable. It seems much likelier that if the liberal culture that we have lived in since the Renaissance comes to an end, the literary art will perish with it.

It seems absurd to predict that the novel itself might one day become extinct, and yet Orwell was right to connect its heyday to the “Protestant centuries.” From Defoe and Fielding to Austen and Dickens, the stuff of the novel was the account of finding one’s place in the world—either through marriage, choice of profession, or journey of inner growth. In feudal Europe, however, one’s place in the world was fixed. Before the novel could come into existence, the specific course of the individual life had to become a thing of interest, and for this to happen, the feudal order had to fall.

Orwell recognized that the novel, like any genre of art, is the product of a particular cultural moment. Each genre reflects certain systems of belief and social structures, and it will flourish with them or shrivel accordingly. At the end of the Middle Ages, passion plays and manuscript illumination withered away; shortly thereafter, the Renaissance court conjured up opera and ballet. This was the last great comprehensive reconfiguration of artistic genres, which has left us with the formal hierarchy in which the triad of painting, sculpture, and architecture represents the most prestigious and significant of the arts. These were the principal arts concerned with the making and adorning of palaces and churches, and the trends they codified soon worked their way down through the decorative arts—ceramics, textiles, and the like. And they maintain their inherited prestige, although the ground beneath their feet has changed dramatically.

Most recently, the great shakeup in the hierarchy of art genres came about through technological innovations. The first major new artistic genres to arise since the age of the absolutist court, photography and the motion picture, are its products. (Perhaps digital art in some form will become the defining new genre of this century, or it might in the end be more important as a means of transmission, like the phonograph record.) These new technologies do not merely add to the repertoire of existing genres, expanding choice in the great human bazaar of entertainment and expression. They also affect the relative position of all other genres, all of which are in competition with one another, for consumers as well as for talented producers.

A new technology can change the cultural moment with shocking speed. America’s culture of vaudeville, vibrant for a half century, sank into oblivion after the introduction of sound in film in 1927. Fred Allen, one vaudeville performer who found a lifeboat in radio, observed with chagrin how theater chains promptly began to bill motion pictures above their vaudeville programs. The big bands of the Swing Era and their culture of nightclubs and ballrooms could not survive television. Now it is literary culture that is on the chopping block. According to Publishers Weekly, the greatest sales of nonfiction books was achieved in 2007—the same year (the magazine might have noted) that Apple introduced the iPhone. Since then, book sales have been declining steadily, up to 10 percent a year.

But technology alone cannot explain what has happened to the fine arts in the past few generations.

The same period has witnessed a catastrophic breakdown of the belief systems that sustained Western civilization. The belief in its goodness and fundamental virtue, once the unspoken premise on which society operated, is something that any high school student, properly instructed, knows how to debunk. The word civilization is endangered, as shown by Google’s Ngram, which tracks the frequency of word usage in print. After 1961, civilization began to be used less and less; by the mid-1980s, its usage had dropped by half. At that very moment, the ironic usage of the word “civilization” started to rise sharply. As Sontag predicted, everything could be put into quotation marks. As goes the word, so goes the concept.

Without a sincere concept of the meaning of civilization, one cannot explain why a masterpiece of Egyptian New Kingdom art counts for more than a creation of 1960s industrial design (other than in dollar value). If one cannot do even that, it is hard to see how one might set out to make serious and lasting art. To make such art—art that refracts the world back to people in some meaningful way, and that illuminates human nature with sympathy and insight—it is not necessary to be a religious believer. Michelangelo certainly was; Leonardo da Vinci certainly was not. But it is necessary to have some sort of larger system of belief, a larger structure of continuity that permits works of art to speak across time. Without such a belief system, all that one can hope for is short-term gain, in the coin of celebrity or notoriety, if not actual coins.

If art is merely the expression of a structure of power, then to dislodge it from its traditional position of prestige in the public square would be a liberation. It would be a similar liberation to dispense with the whole trove of traditional culture—public rituals, folk songs, patriotic fables and myths, true holidays (as opposed to days off from work)—that from the vantage of Foucault might be viewed as oppressive instruments. And indeed they have been largely swept away. But their place has been taken by the institutions of mass media, commerce, and advertising, which exist for the present, are unable to speak across time, and are now occupying much of the terrain formerly held by traditional culture and fine art.

Architecture is the one art that cannot afford to alienate its public—it simply costs too much to build something, and so the architect must still please his sponsor to get his work done—but it, too, has suffered by losing its sense of historical continuity. As it has done so, it approaches the state of advertising—large, eye-catching, memorable objects created by celebrity designers with a signature style. It is no coincidence that when this began to happen, some time in the 1980s, architects stopped working to resolve their buildings—meaning, they ceased their labors over the details and minor features of plan and elevation that would ensure that a building’s parts expressed in miniature the order of the whole. Architects such as Wright and Louis Kahn did not know about DNA when they designed their greatest works—but they understood the principle that the logic of the whole was embedded even in its tiniest element. Virtually no architect does this anymore (apart from a few Japanese masters), and it would be pointless to do so.

The final irony is that the contemporary museum building, the colossal works of freeform sculpture by celebrity designers, may be the only lasting work of art produced in our age. For the making of a great building was once akin to making a fine musical instrument; today the task more nearly resembles the making of a successful billboard.

For a generation or more, the American public has been thoroughly alienated from the life of the fine arts while, paradoxically, continuing to enjoy museums for the sake of sensation and spectacle, much as it enjoyed circuses a century ago. The public accepts that it has nothing important to say about who we are, though it does occasionally rouse itself when something is at stake that does reflect our collective values. When Daniel Libeskind proposed rebuilding the World Trade Center in the academically fashionable deconstructivist style, the public outcry resulted in his being replaced and the construction of a sober and dignified tower. A similar outcry has delayed the building of Frank Gehry’s perversely anti-heroic memorial to Dwight D. Eisenhower, which might yet be realized (never underestimate the power of bureaucratic stealth). But these are exceptions to the pattern of a dormant and indifferent public.

This estrangement has been a disaster for the arts, which need to draw inspiration from the society and culture that is its substrate. It is a myth that an art withdrawn from the realm of public inspection and disapproval is a freer and superior art. The impulse to evade censure can inspire raptures of ingenuity. (The passage of the prim Hays Code in 1932, which led to four decades of censorship in Hollywood, increased the sophistication and wit of American films by a magnitude.) We hear much about art enriching the human experience, which is an agreeable platitude. But it is the other way round. The human experience is needed to enrich art, and without a meaningful living connection to the society that nurtures it, art is a plucked flower.

Chris Burden could never repeat the stunt that made him famous, for the trick about breaking taboos is that you can do it only once. After that, all you can do is endlessly reenact the breaking long after the taboos have gone.

And all that remains is what is broken.

1 I wrote about this exhibition in “Body and Soul” in Commentary’s January 2007 issue.