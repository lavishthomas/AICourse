As AI becomes more and more complex, it can become difficult for even its own designers understand why it acts the way it does. This poses a serious problem, particularly when given the common perception that AI is somehow objective or scientific. In such scenarios, technology becomes a black box that makes decisions and offers pronouncements and that we are encouraged to obey. As complexity only grows, it becomes more and more important to be aware of what effects AI is having on a broader social reality.

But the fact is that technology is never neutral. The example of sexist interpretation of images is just the tip of the iceberg. Since machine learning and AI operate through collecting, filtering, and then learning from and analyzing existing data, they will replicate existing structural biases unless they are designed explicitly to account for and counteract that. To address this situation, an approach would require a specifically social justice-oriented perspective, one that considers how economics intertwine with gender, race, sexuality, and a host of other factors. But given cultural factors such as the pervasive faith in blind meritocracy among tech professionals, pushing ideas focused on equity will be an uphill battle.

As AI becomes more and more complex, it can become difficult for even its own designers understand why it acts the way it does.

There are moves afoot, however. AI Now is a New York-based research initiative led by Kate Crawford and Meredith Whittaker that seeks to further understanding of how AI works, how it might be put to better use, and how current implementations may sometimes be harmful. In an annual report, the group put out a series of recommendations designed to offer a perspective on how to mitigate the worst of AI’s prejudices. Among them: dedicated resources to diversifying the range of inputs for AI systems, especially those related to marginalized groups—photos of men doing the dishes, say, or of two women getting married. Another suggestion is to develop systems to evaluate AI’s fairness and harms when in use; and try and improve diversity amongst the people designing and implementing AI in order to ferret out blind spots and bias. After all, if more of the developers were black, or women, then the programs might not reflect such a white, male worldview.





When we talk about artificial intelligence, it can be easy to get carried away with alarmism. Recently, dramatic headlines circulated that Facebook had to cut off an AI experiment because two bots “invented their own language.” In reality, all that happened is that the bots started to communicate in a shorthand, and not in a way that was understandable to most humans—the program was cancelled not because it had become dangerous, but because it had ceased to be useful. There were no nefarious plots of autonomous intelligences to conquer the earth.

But in a sense, one thing that digital technology has done is to make manifest the ideas of society. Social media, for example, makes visible and concrete the idea of a “public sphere,” and we’ve seen there can be both incredible up- and downsides to this: The electric nature of open conversation also brings with it harassment and public venues for hate. For its part, AI seems to reveal the structure behind the structural bias. There are complex material and ideological factors and institutions that shape our lives, and in order to counteract the prejudice contained therein, we first need to recognize that we are in fact shaped and influenced by these bigger systems, and then work against their tendency to reproduce existing power structures. What is clear, however, is that the “intelligence” of AI is the output of our own investments and biases, and it behoves us to build it in a way the recognizes that fact. AI will always reflect its input, and its input is us—our culture, our ideologies, our selves. To make the technology that will enable a more just future, those are what we have to change.