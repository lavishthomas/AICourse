Top right photo of me working as a street vendor selling dumplings at a construction site and B&W image of an internet cafe. China

When I was researching at Nokia in 2009, which at the time was the world’s largest cellphone company in emerging, I discovered something that I believed challenged their entire business model. After years of conducting ethnographic work in China from living with migrants to working as a street vendor and living in internet cafés, I saw lots of indicators that led me to conclude that low-income consumers were ready to pay for more expensive smartphones. I concluded that Nokia needed to replace their current product development strategy from making expensive smartphones for elite users to affordable smartphones for low-income users. I reported my findings and recommendations to headquarters. But Nokia did not know what to do with my findings. They said my sample size of 100 was weak and small compared to their sample size of several million data points. In addition, they said that there weren’t any signs of my insights in their existing datasets. In response, I told them that it made sense that they haven’t seen any of my data show up in their quantitative datasets because their notion of demand was a fixed quantitative model that didn’t map to how demand worked as a cultural model in China. What is measurable isn’t the same as what is valuable. By now, we all know what happened to Nokia. Microsoft bought them in 2013 and it only has three percent of the global smartphone market. There are many reasons for Nokia’s downfall, but one of the biggest reasons that I witnessed in person was that the company over-relied on numbers. They put a higher value on quantitative data, they didn’t know how to handle data that wasn’t easily measurable, and that didn’t show up in existing reports. What could’ve been their competitive intelligence ended up being their eventual downfall. Since my time at Nokia, I’ve been very perplexed by why organizations value quantitative more than qualitative data. With the rise of Big Data, I’ve seen this process intensify with organizations investing in more big data technology while decreasing budgets for human-centered research. I’m deeply concerned about the future of qualitative, ethnographic work in the Era of Big Data. Ethnographic work has a serious perception problem in a data-driven world. While I’ve always integrated statistical analysis into my qualitative work in academia, I encountered a lot of doubt on the value of ethnographically derived data when I started working primarily with corporations. I started to hear echoes of what Nokia leadership said about my small dataset, that ethnographic data is “small” “petite” “puny.”. What are ethnographers to do when our research is seen as insignificant or invaluable? How can our kind of research be seen as an equally important to algorithmically processed data? To solve this perception problem, ethnographers need a 10 second elevator pitch to a room of data scientists. Lacking the conceptual words to quickly position the value of ethnographic work in the context of Big Data, I have begun, over the last year, to employ the term Thick Data (with a nod to Clifford Geertz!) to advocate for integrative approaches to research. Thick Data is data brought to light using qualitative, ethnographic research methods that uncover people’s emotions, stories, and models of their world. It’s the sticky stuff that’s difficult to quantify. It comes to us in the form of a small sample size and in return we get an incredible depth of meanings and stories. Thick Data is the opposite of Big Data, which is quantitative data at a large scale that involves new technologies around capturing, storing, and analyzing. For Big Data to be analyzable, it must use normalizing, standardizing, defining, clustering, all processes that strips the the data set of context, meaning, and stories. Thick Data can rescue Big Data from the context-loss that comes with the processes of making it usable.