Maybe it was the 23 first-half homers hit by unknown outfielder Adam Duvall. Maybe it was when Giancarlo Stanton kept making Chris Berman say “back-back-back,” or when Bartolo Colon broke into a 30.6 second trot around the bases. But at some point this season, you’ve probably noticed: We haven’t just gone back to those homer-happy days of 1999 and 2000 — we’ve surpassed them.

In 2016, the typical major league plate appearance is more likely to result in a homer than ever before. And this onslaught happened quickly: Home runs on contact—the rate at which non-strikeout at-bats produce dingers — is up 35 percent compared with 2014, which has helped drive a scoring increase of 0.41 runs per team, per game.

Naturally, many fans are wondering why. (The rest have already decided it’s steroids.) And while we can’t quite settle the speculation, we can offer the strongest statistical evidence yet that it isn’t because of the batters, or the pitchers, or the ballparks, or even performance-enhancing drugs. Instead, the numbers suggest the ball itself is to blame.

The research so far

Just before Opening Day, we investigated the unprecedented uptick in the home run rate we’d observed in the second half of last season, which held up into the playoffs and spring training. We found that the increase was much too large to have happened by chance, and we also discovered that the rise in home run rate coincided with a significant increase in batted ball speed, which (as physicist Alan Nathan later showed) was sufficient to explain the extra homers. But that led to a new, equally perplexing question: Why were balls being hit harder?

We knew there was no change to the strike zone, and we even adjusted for warmer weather, an influx of young power hitters and the Reds’ pitching staff (yes, it was terrible in 2015 too). We still found that exit velocities were higher than they “should” have been based on previous performance, and that roughly 30 percent more home runs had been hit than expected.

That left the baseball. So we bought a dozen official “Bud Selig”-branded balls from 2014 and a dozen “Rob Manfred”-branded balls from the second half of 2015, and we sent them to the Sports Science Laboratory at Washington State University. There, they were fired from a high-speed air cannon at a steel plate so that their coefficient of restitution — basically, their bounciness — could be measured precisely. But “Steel-Plategate” didn’t catch on: while the tests did detect a small increase in bounciness, variation between individual balls was so high that the overall change wasn’t statistically significant.

We couldn’t confirm the juiced-ball hypothesis, but we also couldn’t dismiss it. We weren’t testing game balls, and one batch wasn’t a large enough sample to be certain that the balls hadn’t changed. And although some readers responded to our article by proposing performance-enhancing drugs as a possible culprit, that explanation didn’t seem consistent with a sudden, dramatic mid-season change. To believe that PEDs are responsible, one would also have to believe that during the 2015 All-Star break, the whole league decided in unison to start taking a powerful, undetectable substance that helped only hitters. That seems like a stretch.

Everyone has a hypothesis

But the dingers keep coming for hitters of all ages, and the league keeps insisting that the ball hasn’t changed, so players and analysts keep coming up with ideas — most of which were summarized by Tyler Kepner in The New York Times last week.

As those theories proposed, maybe teams are so obsessed with velocity that they’ve neglected command and secondary stuff in developing pitchers, making them easier to take deep. Maybe hitters have learned to time harder fastballs, or focused on hitting fly balls to avoid infield shifts, or decided to stop worrying about making contact and start swinging for the fences, or realized how adept pitchers are at putting them away with two strikes and learned to swing more often in earlier counts. Or maybe, as Commissioner Manfred suggested last week, teams have gotten smarter about batting better hitters high in the lineup, leading to more plate appearances for power guys.

Many of these conjectures don’t stand up to the evidence. Fly balls aren’t becoming more common (although some players’ launch angles have improved ); an emphasis on higher velocities hasn’t led to pitchers grooving more pitches, abandoning slow stuff, or throwing higher in the zone ; the best home run hitters aren’t getting more plate appearances than they were in 2014; and while strikeout rates are up this year, they didn’t climb last season, when the home run rate first spiked.

More broadly, these and other explanations suffer from the same problem as the PED theory: Baseball doesn’t have a hive mind, and these sweeping changes would have had to be carefully choreographed across the league to produce such a noticeable and abrupt midsummer surge. Occam’s razor keeps pointing back to the baseball.

Major answers in the minor leagues

Unfortunately, laboratory ball-testing is extremely expensive. And even if we did test enough balls to overcome the random variation between batches and get a reliable result, we still couldn’t be sure that the balls we were buying were identical to the ones used in major league games.

But Major League Baseball does come with a built-in control group: the minor leagues. With a few exceptions, major league players were once minor league players. And there’s a lot of recent overlap between levels: 55 percent of the hitters who’ve made a major league plate appearance since the start of last season have also made a Triple-A plate appearance over the same span. If the huge hike in home run rate we’ve seen in the majors has stemmed from widespread trends in player development and player approach, then we would expect to see some of the same effects among minor league players, especially at the high levels that many major leaguers just left.

However, that hasn’t been the case. The chart below displays the annual rates of home runs on contact in the majors (red) and Triple-A (blue), going back to 1990:

During this period of 25-plus years, there was a solid correlation between home run rates in MLB and Triple-A. Although the major league home run rate in any given year was always higher (in part because big leaguers swing and — with apologies to Michael Kopech — throw harder, on average), the two rates tended to move in tandem: high-homer seasons in the majors were high-homer seasons in Triple-A, too.

Now that’s no longer true. As the MLB home run rate jumped last year, the Triple-A rate fell to its lowest level in years, and as the MLB rate has skyrocketed to an all-time high in 2016, the Triple-A rate has bounced back only slightly, staying lower than it was during the first half of the decade. The gap between this year’s major league and Triple-A home run rates is the biggest of any season in our sample, and 57 percent larger than the average annual difference.

(This also applies to combined home run rates at all minor league levels: Historically, minor league and major league home run rates have tracked quite closely, but in the past calendar year, they’ve diverged dramatically.)

A natural experiment

That finding alone is suggestive, but there’s another way we can examine this. As we noted above, it’s very common for hitters to go back and forth between the big leagues and Triple-A. So rather than look at all major leaguers and all Triple-A players, we can focus only on the subset of players who’ve appeared in both Triple-A and the majors since the 2015 All-Star break.

Of the players who had at least one contacted ball in both Triple-A and MLB last season after the All-Star break, about 3.4 percent of their contact resulted in home runs. But the rates were very different depending on which league they were in: In Triple-A, only 3.0 percent of their contact made it over the fence; in MLB, that rose to 3.8 percent, about 27 percent higher. Earlier in the year — before the All-Star break — the difference in home run rates had been about half that, with Triple-A coming in unchanged at 3.0 percent and MLB at only 3.4 percent. This year’s rates are similar to those from the second half of last season: 3.1 percent in Triple-A, and 3.9 percent in the majors.

To keep conditions even more consistent, we can also drill down further, looking for head-to-head matchups between batters and pitchers who faced each other in both Triple-A and MLB since the 2015 All-Star break. In total, there were some 3,586 batted balls with both the same pitcher and the same hitter, divided roughly evenly between MLB and Triple-A. In those matchups, 3.3 percent of contact became home runs in Triple-A, and 4.3 percent became homers in MLB, a 30 percent difference that is very close to what our previous research showed.

Of course, players who switch between Triple-A and MLB are not drawn from a random sample, and we also have to deal with the complicating problem of different ballparks. So we accounted for those factors, building a model to explain whether each case of contact would see the ball driven over the fence, and we measured whether the difference in home run probability between Triple-A and the majors was statistically significant.

For both the league-switchers and the individual matchups, we found that the level at which a player hit was a big factor in determining whether his batted balls would become home runs. In both cases, hitters knocked roughly 30 percent more balls out of the park in the majors than expected. And if we focus the model solely on homers as a percentage of fly-ball contact — which is also at an all-time high in the majors — the results stay the same.

Those findings roughly correspond to what we see in the exit-velocity data. Compared with 2014, the average speed off an MLB bat this season is up approximately 1.4 mph; the average speeds for the four full-season minor league levels have also increased over the same period, but only by roughly half as much.

Complicating matters somewhat is that exit speeds haven’t increased uniformly at every launch angle. According to TrackMan, the average exit speed of balls hit at 0 to 10 degrees (essentially, low line drives) hasn’t changed, while exit speeds have climbed noticeably in every other range. Batted balls at 10 degrees or lower don’t turn into homers no matter how hard they’re hit, but we would still expect them to be affected by whatever is causing the increases at other angles. Barring measurement error, that’s the one piece of statistical evidence that argues against the baseball being bouncier.

But the balance of the evidence leans in the opposite direction. If the matchups between exactly the same hitters and pitchers are yielding many more homers in MLB than Triple-A, and we’ve accounted for ballpark effects… well, what does that leave? Once again, we come back to the ball.

Different balls, made in different places

According to representatives of both Major League Baseball and Rawlings, MLB balls are manufactured in Costa Rica, while all other balls — including the minor-league model — are manufactured in China. According to an MLB spokesman, balls from both sources are “tested extensively and play relatively the same,” although a Rawlings rep notes that the seams on minor league balls are “slightly flatter and not as raised” (which we would actually expect to make them fly farther, resulting in more home runs in the minors). A 2000 study commissioned by MLB and conducted by the league’s official testing facility, the UMass-Lowell Baseball Research Center, reported that the minor-league ball had a less-compressed cork center, which decreased its carry. But while that could help account for the long-running historical difference in home run rates across levels, it can’t explain the way that difference has grown in recent seasons.

One thing that we know sets the majors and minors apart is the ball. Major- and minor-league balls are produced at different facilities in different countries approximately 10,000 miles apart, which makes it more plausible that something about the MLB ball could have changed without the minor-league ball being affected.

Conspiracy theorists — some major league players among them — need not squint hard to connect the dots: In 2014, scoring sank to its lowest full-season level since 1976, and restoring offense was a hot topic over the following winter. Fox Sports’ Ken Rosenthal reported in January 2015 that the league had recently sent the MLB Players Association a packet of ideas for bolstering scoring, one of which was “wrapping the ball tighter to make it fly farther.” While that wasn’t a formal proposal, it seems somewhat convenient that balls did start flying farther so soon, particularly in a way that would be consistent with an altered ball being introduced at the All-Star break, when some teams replenish their supply. This wouldn’t be the first time that the baseball’s guts have been quietly tweaked, or that a livelier ball has been introduced on the sly at midseason. And there is recent precedent for a similar ball-and-switch in a high-level league, where (as Rob Manfred is aware) things didn’t end well for its commissioner.

Of course, a difference in the ball need not be something that the league is aware of. Baseballs are intricate objects, and a small change to the manufacturing process at Rawlings’ Costa Rican factory — where working conditions are far from ideal — could have a big impact on baseball’s delicate pitcher-batter balance, intentional or otherwise. (One change we’re aware of: Rawlings laid off 200 Costa Rican employees last summer when it moved its uniform-manufacturing operation to El Salvador to slash costs.) Because slight increases in bounciness can lead to big boosts in batted-ball speed and distance — and because MLB allows a fairly large range of restitution coefficients — it’s possible that balls could get springier while still meeting the league’s standards. Even the UMass-Lowell study from 2000 reported that “theoretically, two baseballs could meet the specifications but one ball could be hit 49.1 feet further than the other could be hit.”

To be clear, a juiced ball wouldn’t be an inherently bad thing. In fact, if the ball is juiced, the sport might be better off because of it. It’s getting harder and harder for hitters to make contact, so if the contact they do make were having a smaller effect, scoring could really be stuck in the doldrums. That said, it would still be nice to know when there’s a change in something as fundamental to baseball as, well, the baseball.

But although our results are suggestive, they’re not conclusive. To make a more compelling case, we’d need to conduct additional lab tests and start doing dissections, although those might also leave us looking for answers. (We’re unlikely to slice open a baseball and find a Super Ball inside.) According to one hitter we spoke to, who’s played in both the majors and Triple-A since the homer explosion started, the juiced-ball theory “seems to make sense and there is no doubt in my mind that there is a difference in the balls [between Triple-A and the majors]. I just wouldn’t want to get so tunnel vision about the balls [that] we could be missing a bigger issue.”

He’s right to reserve judgment; one thing we’ve learned from this mystery is that even when armed with big data, we can’t declare every case closed. But the deeper we dig into baseball’s record home run rate, the harder it is to believe that it doesn’t have something to do with the ball.