Scientism, in this good sense, is not the belief that members of the occupational guild called “science” are particularly wise or noble. On the contrary, the defining practices of science, including open debate, peer review, and double-blind methods, are explicitly designed to circumvent the errors and sins to which scientists, being human, are vulnerable. Scientism does not mean that all current scientific hypotheses are true; most new ones are not, since the cycle of conjecture and refutation is the lifeblood of science. It is not an imperialistic drive to occupy the humanities; the promise of science is to enrich and diversify the intellectual tools of humanistic scholarship, not to obliterate them. And it is not the dogma that physical stuff is the only thing that exists. Scientists themselves are immersed in the ethereal medium of information, including the truths of mathematics, the logic of their theories, and the values that guide their enterprise. In this conception, science is of a piece with philosophy, reason, and Enlightenment humanism. It is distinguished by an explicit commitment to two ideals, and it is these that scientism seeks to export to the rest of intellectual life.





The first is that the world is intelligible. The phenomena we experience may be explained by principles that are more general than the phenomena themselves. These principles may in turn be explained by more fundamental principles, and so on. In making sense of our world, there should be few occasions in which we are forced to concede “It just is” or “It’s magic” or “Because I said so.” The commitment to intelligibility is not a matter of brute faith, but gradually validates itself as more and more of the world becomes explicable in scientific terms. The processes of life, for example, used to be attributed to a mysterious élan vital; now we know they are powered by chemical and physical reactions among complex molecules.

Demonizers of scientism often confuse intelligibility with a sin called reductionism. But to explain a complex happening in terms of deeper principles is not to discard its richness. No sane thinker would try to explain World War I in the language of physics, chemistry, and biology as opposed to the more perspicuous language of the perceptions and goals of leaders in 1914 Europe. At the same time, a curious person can legitimately ask why human minds are apt to have such perceptions and goals, including the tribalism, overconfidence, and sense of honor that fell into a deadly combination at that historical moment.

Many of our cultural institutions cultivate a philistine indifference to science.

The second ideal is that the acquisition of knowledge is hard. The world does not go out of its way to reveal its workings, and even if it did, our minds are prone to illusions, fallacies, and super- stitions. Most of the traditional causes of belief—faith, revelation, dogma, authority, charisma, conventional wisdom, the invigorating glow of subjective certainty—are generators of error and should be dismissed as sources of knowledge. To understand the world, we must cultivate work-arounds for our cognitive limitations, including skepticism, open debate, formal precision, and empirical tests, often requiring feats of ingenuity. Any movement that calls itself “scientific” but fails to nurture opportunities for the falsification of its own beliefs (most obviously when it murders or imprisons the people who disagree with it) is not a scientific movement.

In which ways, then, does science illuminate human affairs? Let me start with the most ambitious: the deepest questions about who we are, where we came from, and how we define the meaning and purpose of our lives. This is the traditional territory of religion, and its defenders tend to be the most excitable critics of scientism. They are apt to endorse the partition plan proposed by Stephen Jay Gould in his worst book, Rocks of Ages, according to which the proper concerns of science and religion belong to “non-overlapping magisteria.” Science gets the empirical universe; religion gets the questions of moral meaning and value.

Unfortunately, this entente unravels as soon as you begin to examine it. The moral worldview of any scientifically literate person—one who is not blinkered by fundamentalism—requires a radical break from religious conceptions of meaning and value.

To begin with, the findings of science entail that the belief systems of all the world’s traditional religions and cultures—their theories of the origins of life, humans, and societies—are factually mistaken. We know, but our ancestors did not, that humans belong to a single species of African primate that developed agriculture, government, and writing late in its history. We know that our species is a tiny twig of a genealogical tree that embraces all living things and that emerged from prebiotic chemicals almost four billion years ago. We know that we live on a planet that revolves around one of a hundred billion stars in our galaxy, which is one of a hundred billion galaxies in a 13.8-billion-year-old universe, possibly one of a vast number of universes. We know that our intuitions about space, time, matter, and causation are incommensurable with the nature of reality on scales that are very large and very small. We know that the laws governing the physical world (including accidents, disease, and other misfortunes) have no goals that pertain to human well-being. There is no such thing as fate, providence, karma, spells, curses, augury, divine retribution, or answered prayers—though the discrepancy between the laws of probability and the workings of cognition may explain why people believe there are. And we know that we did not always know these things, that the beloved convictions of every time and culture may be decisively falsified, doubtless including some we hold today.

In other words, the worldview that guides the moral and spiritual values of an educated person today is the worldview given to us by science. Though the scientific facts do not by themselves dictate values, they certainly hem in the possibilities. By stripping ecclesiastical authority of its credibility on factual matters, they cast doubt on its claims to certitude in matters of morality. The scientific refutation of the theory of vengeful gods and occult forces undermines practices such as human sacrifice, witch hunts, faith healing, trial by ordeal, and the persecution of heretics. The facts of science, by exposing the absence of purpose in the laws governing the universe, force us to take responsibility for the welfare of ourselves, our species, and our planet. For the same reason, they undercut any moral or political system based on mystical forces, quests, destinies, dialectics, struggles, or messianic ages. And in combination with a few unexceptionable convictions— that all of us value our own welfare and that we are social beings who impinge on each other and can negotiate codes of conduct—the scientific facts militate toward a defensible morality, namely adhering to principles that maximize the flourishing of humans and other sentient beings. This humanism, which is inextricable from a scientific understanding of the world, is becoming the de facto morality of modern democracies, international organizations, and liberalizing religions, and its unfulfilled promises define the moral imperatives we face today.

Moreover, science has contributed—directly and enormously—to the fulfillment of these values. If one were to list the proudest accomplishments of our species (setting aside the removal of obstacles we set in our own path, such as the abolition of slavery and the defeat of fascism), many would be gifts bestowed by science.

The most obvious is the exhilarating achievement of scientific knowledge itself. We can say much about the history of the universe, the forces that make it tick, the stuff we’re made of, the origin of living things, and the machinery of life, including our own mental life. Better still, this understanding consists not in a mere listing of facts, but in deep and elegant principles, like the insight that life depends on a molecule that carries information, directs metabolism, and replicates itself.

Science has also provided the world with images of sublime beauty: stroboscopically frozen motion, exotic organisms, distant galaxies and outer planets, fluorescing neural circuitry, and a luminous planet Earth rising above the moon’s horizon into the blackness of space. Like great works of art, these are not just pretty pictures but prods to contemplation, which deepen our understanding of what it means to be human and of our place in nature.

And contrary to the widespread canard that technology has created a dystopia of deprivation and violence, every global measure of human flourishing is on the rise. The numbers show that after millennia of near-universal poverty, a steadily growing proportion of humanity is surviving the first year of life, going to school, voting in democracies, living in peace, communicating on cell phones, enjoying small luxuries, and surviving to old age. The Green Revolution in agronomy alone saved a billion people from starvation. And if you want examples of true moral greatness, go to Wikipedia and look up the entries for “smallpox” and “rinderpest” (cattle plague). The definitions are in the past tense, indicating that human ingenuity has eradicated two of the cruelest causes of suffering in the history of our kind.

Though science is beneficially embedded in our material, moral, and intellectual lives, many of our cultural institutions, including the liberal arts programs of many universities, cultivate a philistine indifference to science that shades into contempt. Students can graduate from elite colleges with a trifling exposure to science. They are commonly misinformed that scientists no longer care about truth but merely chase the fashions of shifting paradigms. A demonization campaign anachronistically impugns science for crimes that are as old as civilization, including racism, slavery, conquest, and genocide.

Just as common, and as historically illiterate, is the blaming of science for political movements with a pseudoscientific patina, particularly Social Darwinism and eugenics. Social Darwinism was the misnamed laissez-faire philosophy of Herbert Spencer. It was inspired not by Darwin’s theory of natural selection, but by Spencer’s Victorian-era conception of a mysterious natural force for progress, which was best left unimpeded. Today the term is often used to smear any application of evolution to the understanding of human beings. Eugenics was the campaign, popular among leftists and progressives in the early decades of the twentieth century, for the ultimate form of social progress, improving the genetic stock of humanity. Today the term is commonly used to assail behavioral genetics, the study of the genetic contributions to individual differences.

I can testify that this recrimination is not a relic of the 1990s science wars. When Harvard reformed its general education requirement in 2006 to 2007, the preliminary task force report introduced the teaching of science without any mention of its place in human knowledge: “Science and technology directly affect our students in many ways, both positive and negative: they have led to life-saving medicines, the internet, more efficient energy storage, and digital entertainment; they also have shepherded nuclear weapons, biological warfare agents, electronic eavesdropping, and damage to the environment.” This strange equivocation between the utilitarian and the nefarious was not applied to other disciplines. (Just imagine motivating the study of classical music by noting that it both generates economic activity and inspired the Nazis.) And there was no acknowledgment that we might have good reasons to prefer science and know-how over ignorance and superstition.

At a 2011 conference, another colleague summed up what she thought was the mixed legacy of science: the eradication of smallpox on the one hand; the Tuskegee syphilis study on the other. (In that study, another bloody shirt in the standard narrative about the evils of science, public-health researchers beginning in 1932 tracked the progression of untreated, latent syphilis in a sample of impoverished African Americans.) The comparison is obtuse. It assumes that the study was the unavoidable dark side of scientific progress as opposed to a universally deplored breach, and it compares a one-time failure to prevent harm to a few dozen people with the prevention of hundreds of millions of deaths per century, in perpetuity.

A major goad for the recent denunciations of scientism has been the application of neuroscience, evolution, and genetics to human affairs. Certainly many of these applications are glib or wrong, and they are fair game for criticism: scanning the brains of voters as they look at politicians’ faces, attributing war to a gene for aggression, explaining religion as an evolutionary adaptation to bond the group. Yet it’s not unheard of for intellectuals who are innocent of science to advance ideas that are glib or wrong, and no one is calling for humanities scholars to go back to their carrels and stay out of discussions of things that matter. It is a mistake to use a few wrongheaded examples as an excuse to quarantine the sciences of human nature from our attempt to understand the human condition.

To simplify is not to be simplistic.

Take our understanding of politics. “What is government itself,” asked James Madison, “but the greatest of all reflections on human nature?” The new sciences of the mind are reexamining the connections between politics and human nature, which were avidly discussed in Madison’s time but submerged during a long interlude in which humans were assumed to be blank slates or rational actors. Humans, we are increasingly appreciating, are moralistic actors, guided by norms and taboos about authority, tribe, and purity, and driven by conflicting inclinations toward revenge and reconciliation. These impulses ordinarily operate beneath our conscious awareness, but in some circumstances they can be turned around by reason and debate. We are starting to grasp why these moralistic impulses evolved; how they are implemented in the brain; how they differ among individuals, cultures, and sub- cultures; and which conditions turn them on and off.

The application of science to politics not only enriches our stock of ideas, but also offers the means to ascertain which of them are likely to be correct. Political debates have traditionally been deliberated through case studies, rhetoric, and what software engineers call HiPPO (highest-paid person’s opinion). Not surprisingly, the controversies have careened without resolution. Do democracies fight each other? What about trading partners? Do neighboring ethnic groups inevitably play out ancient hatreds in bloody conflict? Do peacekeeping forces really keep the peace? Do terrorist organizations get what they want? How about Gandhian nonviolent movements? Are post-conflict reconciliation rituals effective at preventing the renewal of conflict?

History nerds can adduce examples that support either answer, but that does not mean the questions are irresolvable. Political events are buffeted by many forces, so it’s possible that a given force is potent in general but submerged in a particular instance. With the advent of data science—the analysis of large, open-access data sets of numbers or text—signals can be extracted from the noise and debates in history and political science resolved more objectively. As best we can tell at present, the answers to the questions listed above are (on average, and all things being equal) no, no, no, yes, no, yes, and yes.

The humanities are the domain in which the intrusion of science has produced the strongest recoil. Yet it is just that domain that would seem to be most in need of an infusion of new ideas. By most accounts, the humanities are in trouble. University programs are downsizing, the next generation of scholars is un- or underemployed, morale is sinking, students are staying away in droves. No thinking person should be indifferent to our society’s disinvestment from the humanities, which are indispensable to a civilized democracy.

Diagnoses of the malaise of the humanities rightly point to anti-intellectual trends in our culture and to the commercialization of our universities. But an honest appraisal would have to acknowledge that some of the damage is self-inflicted. The humanities have yet to recover from the disaster of postmodernism, with its defiant obscurantism, dogmatic relativism, and suffocating political correctness. And they have failed to define a progressive agenda. Several university presidents and provosts have lamented to me that when a scientist comes into their office, it’s to announce some exciting new research opportunity and demand the resources to pursue it. When a humanities scholar drops by, it’s to plead for respect for the way things have always been done.

Those ways do deserve respect, and there can be no replacement for the varieties of close reading, thick description, and deep immersion that erudite scholars can apply to individual works. But must these be the only paths to understanding? A consilience with science offers the humanities countless possibilities for innovation in understanding. Art, culture, and society are products of human brains. They originate in our faculties of perception, thought, and emotion, and they cumulate and spread through the epidemiological dynamics by which one person affects others. Shouldn’t we be curious to understand these connections? Both sides would win. The humanities would enjoy more of the explanatory depth of the sciences, to say nothing of the kind of a progressive agenda that appeals to deans and donors. The sciences could challenge their theories with the natural experiments and ecologically valid phenomena that have been so richly characterized by humanists.

In some disciplines, this consilience is a fait accompli. Archeology has grown from a branch of art history to a high-tech science. Linguistics and the philosophy of mind shade into cognitive science and neuroscience.

Similar opportunities are there for the exploring. The visual arts could avail themselves of the explosion of knowledge in vision science, including the perception of color, shape, texture, and lighting, and the evolutionary aesthetics of faces and landscapes. Music scholars have much to discuss with the scientists who study the perception of speech and the brain’s analysis of the auditory world.

As for literary scholarship, where to begin? John Dryden wrote that a work of fiction is “a just and lively image of human nature, representing its passions and humours, and the changes of fortune to which it is subject, for the delight and instruction of mankind.” Linguistics can illuminate the resources of grammar and discourse that allow authors to manipulate a reader’s imaginary experience. Cognitive psychology can provide insight about readers’ ability to reconcile their own consciousness with those of the author and characters. Behavioral genetics can update folk theories of parental influence with discoveries about the effects of genes, peers, and chance, which have profound implications for the interpretation of biography and memoir—an endeavor that also has much to learn from the cognitive psychology of memory and the social psychology of self-presentation. Evolutionary psychologists can distinguish the obsessions that are universal from those that are exaggerated by a particular culture and can lay out the inherent conflicts and confluences of interest within families, couples, friendships, and rivalries that are the drivers of plot.

And as with politics, the advent of data science applied to books, periodicals, correspondence, and musical scores holds the promise for an expansive new “digital humanities.” The possibilities for theory and discovery are limited only by the imagination and include the origin and spread of ideas, networks of intellectual and artistic influence, the persistence of historical memory, the waxing and waning of themes in literature, and patterns of unofficial censorship and taboo.

Nonetheless, many humanities scholars have reacted to these opportunities like the protagonist of the grammar-book example of the volitional future tense: “I will drown; no one shall save me.” Noting that these analyses flatten the richness of individual works, they reach for the usual adjectives: simplistic, reductionist, naïve, vulgar, and of course, scientistic.

The complaint about simplification is misbegotten. To explain something is to subsume it under more general principles, which always entails a degree of simplification. Yet to simplify is not to be simplistic. An appreciation of the particulars of a work can co-exist with explanations at many other levels, from the personality of an author to the cultural milieu, the faculties of human nature, and the laws governing social beings. The rejection of a search for general trends and principles calls to mind Jorge Luis Borges’s fictitious empire in which “the Cartographers Guild drew a map of the Empire whose size was that of the Empire, coinciding point for point with it. The following Generations ... saw the vast Map to be Useless and permitted it to decay and fray under the Sun and winters.”

And the critics should be careful with the adjectives. If anything is naïve and simplistic, it is the conviction that the legacy silos of academia should be fortified and that we should be forever content with current ways of making sense of the world. Surely our conceptions of politics, culture, and morality have much to learn from our best understanding of the physical universe and of our makeup as a species.

Steven Pinker is a contributing editor at The New Republic, the Johnstone Family Professor of Psychology at Harvard University, and the author, most recently, of The Better Angels of our Nature: Why Violence Has Declined.